{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e35b2745cb4d25ad88ca52135f75ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Load index', index=2, layout=Layout(width='300px'), options=(('WTI 7CULT …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "#import bokeh, bokeh.plotting, bokeh.models, \n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_state as treaty_repository\n",
    "\n",
    "import textacy.keyterms\n",
    "\n",
    "#from beakerx.object import beakerx\n",
    "#from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "treaty_repository.load_wti_index_with_gui(data_folder=config.DATA_FOLDER)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 17:03:12,322 : ERROR : <ipython-input-4-307e43204440>.<module>() : Invalid selection: value not found\n"
     ]
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(config.DATA_FOLDER, treaty_repository.current_wti_index(), container)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms <span style='float: right; color: green'>OPTIONAL</span>\n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n",
    "- [RAKE]\tRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Son\n",
    "https://github.com/csurfer/rake-nltk\n",
    "https://github.com/aneesha/RAKE\n",
    "https://github.com/vgrabovets/multi_rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>RAKE <span style='float: right; color: green'>WORK IN PROGRESS</span>\n",
    "\n",
    "https://github.com/JRC1995/RAKE-Keyword-Extraction\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Document Key Terms\n",
    "from rake_nltk import Rake, Metric\n",
    "import string\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import gui_utility\n",
    "import types\n",
    "\n",
    "def textacy_rake(doc, language='english', normalize='lemma', n_keyterms=20, stopwords=None, metric=Metric.DEGREE_TO_FREQUENCY_RATIO):\n",
    "    punctuations = string.punctuation + \"\\\"\"\n",
    "    r = Rake(\n",
    "        stopwords=stopwords,  # NLTK stopwords if None\n",
    "        punctuations=punctuations, # NLTK by default\n",
    "        language=language,\n",
    "        ranking_metric=metric,\n",
    "        max_length=100000,\n",
    "        min_length=1\n",
    "    )\n",
    "    text = ' '.join([ x.lemma_.lower().strip('_') for x in doc ] if normalize == 'lemma' else [ x.lower_ for x in doc.spacy_doc ])\n",
    "    r.extract_keywords_from_text(doc.text)\n",
    "    keyterms = [ (y, x) for (x, y) in r.get_ranked_phrases_with_scores() ]\n",
    "    return keyterms[:n_keyterms]\n",
    "\n",
    "\n",
    "def display_rake_gui(corpus, language):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    \n",
    "    document_options = gui_utility.get_treaty_dropdown_options(treaty_repository.current_wti_index(), corpus)\n",
    "    metric_options = [\n",
    "        ('Degree / Frequency', Metric.DEGREE_TO_FREQUENCY_RATIO),\n",
    "        ('Degree', Metric.WORD_DEGREE),\n",
    "        ('Frequency', Metric.WORD_FREQUENCY)\n",
    "    ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=lw(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=10, step=1, layout=lw(width='340px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=lw(width='40%')),\n",
    "        metric=widgets.Dropdown(description='Metric', options=metric_options, value=Metric.DEGREE_TO_FREQUENCY_RATIO, layout=lw(width='300px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=lw(width='160px')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=lw('40px')),\n",
    "    )\n",
    "    \n",
    "    def compute_textacy_rake(corpus, treaty_id, language, normalize, n_keyterms, metric):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            doc = textacy_utility.get_document_by_id(corpus, treaty_id)\n",
    "            phrases = textacy_rake(doc, language=language, normalize=normalize, n_keyterms=n_keyterms, stopwords=None, metric=metric)\n",
    "            df = pd.DataFrame(phrases, columns=['phrase', 'score'])\n",
    "            display(df.set_index('phrase'))\n",
    "            return df\n",
    "    \n",
    "    itw = widgets.interactive(\n",
    "        compute_textacy_rake,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=gui.document_id,\n",
    "        language=widgets.fixed(language),\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "        metric=gui.metric\n",
    "    )\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right, gui.metric, gui.normalize]),\n",
    "        widgets.HBox([gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "try:\n",
    "    display_rake_gui(current_corpus(), language='english')\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color: green'>PREPARE/DESCRIBE </span>TextRank/SingleRank <span style='float: right; color: green'>OPTIONAL</span>\n",
    "\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy.keyterms\n",
    "\n",
    "def display_document_key_terms_gui(corpus, wti_index):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    \n",
    "    methods = { 'RAKE': textacy_rake, 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)    \n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=lw(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=lw('240px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[0][1], layout=lw(width='40%')),        \n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'RAKE', 'TextRank', 'SingleRank' ], value='TextRank', layout=lw('180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=lw('160px')),\n",
    "        left=widgets.Button(description='<<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>>', button_style='Success', layout=lw('40px')),\n",
    "    )\n",
    "    \n",
    "    def get_keyterms(method, doc, normalize, n_keyterms):\n",
    "        keyterms = methods[method](doc, normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ', '.join([ x for x, y in keyterms ])\n",
    "        gui.progress.value += 1\n",
    "        return terms\n",
    "    \n",
    "    def get_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        treaty_ids = [ document_id ] if document_id is not None else [ doc._.meta['treaty_id'] for doc in corpus ]\n",
    "        gui.progress.value = 0\n",
    "        gui.progress.max = len(treaty_ids)\n",
    "        keyterms = [\n",
    "            get_keyterms(method, textacy_utility.get_document_by_id(corpus, treaty_id), normalize, n_keyterms) for treaty_id in treaty_ids\n",
    "        ]\n",
    "        df = pd.DataFrame({ 'treaty_id': treaty_ids, 'keyterms': keyterms}).set_index('treaty_id')\n",
    "        \n",
    "        # Add parties and groups\n",
    "        df_extended = pd.merge(df, wti_index.treaties, left_index=True, right_index=True, how='inner')\n",
    "        group_map = wti_index.get_parties()['group_name'].to_dict()        \n",
    "        df_extended['group1'] = df_extended['party1'].map(group_map)\n",
    "        df_extended['group2'] = df_extended['party2'].map(group_map)        \n",
    "        columns = ['signed_year', 'party1', 'group1', 'party2', 'group2', 'keyterms']\n",
    "        \n",
    "        gui.progress.value = 0\n",
    "        return df_extended[columns]\n",
    "\n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = get_document_key_terms(corpus, method, document_id, normalize, n_keyterms)\n",
    "            if len(df) == 1:\n",
    "                print(df.iloc[0]['keyterms'])\n",
    "            else:\n",
    "                display(df)\n",
    "            \n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.left, gui.right, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "try:\n",
    "    display_document_key_terms_gui(current_corpus(), treaty_repository.current_wti_index())\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "def display_cleaned_up_text(container, gui, display_type, treaty_id, gpe_substitutions, word_count_scores, word_document_count_scores, **opts): \n",
    "       \n",
    "    try:\n",
    "        def get_merged_words(scores, low, high):\n",
    "            ws = set([])\n",
    "            for x, wl in scores.items():\n",
    "                if low <= x <= high:\n",
    "                    ws.update(wl)\n",
    "            return ws\n",
    "\n",
    "        corpus = container.textacy_corpus\n",
    "                \n",
    "        doc = textacy_utility.get_document_by_id(corpus, treaty_id)\n",
    "\n",
    "        if doc is None:\n",
    "            return\n",
    "        \n",
    "        gui.output_text.clear_output()\n",
    "        gui.output_statistics.clear_output()\n",
    "\n",
    "        if display_type.startswith('source_text'):\n",
    "\n",
    "            source_files = {\n",
    "                'source_text_raw': { 'filename': container.source_path, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "                'source_text_edited': { 'filename': container.source_path, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "                'source_text_preprocessed': { 'filename': container.prepped_source_path, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "            }        \n",
    "\n",
    "            source_filename = source_files[display_type]['filename']\n",
    "            description =  source_files[display_type]['description']\n",
    "            text = utility.zip_get_text(source_filename, doc._.meta['filename'])\n",
    "\n",
    "            with gui.output_text:\n",
    "                print('[ ' + description.upper() + ' ]')\n",
    "                print(text)\n",
    "            return\n",
    "\n",
    "        with gui.output_text:\n",
    "\n",
    "            normalize = opts['normalize'] or 'orth'\n",
    "            \n",
    "            extra_stop_words = set([])            \n",
    "            \n",
    "            if opts['min_freq'] > 1:\n",
    "                extra_stop_words.update(get_merged_words(word_count_scores[normalize], 1, opts['min_freq']))\n",
    "            \n",
    "            if opts['max_doc_freq'] < 100:                \n",
    "                extra_stop_words.update(get_merged_words(word_document_count_scores[normalize], opts['max_doc_freq'], 100))            \n",
    "            \n",
    "            extract_args = dict(\n",
    "                args=dict(\n",
    "                    ngrams=opts['ngrams'],\n",
    "                    named_entities=opts['named_entities'],\n",
    "                    normalize=opts['normalize'],\n",
    "                    as_strings=True\n",
    "                ),\n",
    "                kwargs=dict(\n",
    "                    min_freq=opts['min_freq'],\n",
    "                    include_pos=opts['include_pos'],\n",
    "                    filter_stops=opts['filter_stops'],\n",
    "                    filter_punct=opts['filter_punct']\n",
    "                ),\n",
    "                min_length=opts['min_word'],\n",
    "                extra_stop_words=extra_stop_words,\n",
    "                substitutions=(gpe_substitutions if opts['mask_gpe'] else None),\n",
    "            )\n",
    "            \n",
    "            terms = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n",
    "\n",
    "            if len(terms) == 0:\n",
    "                print(\"No text. Please change selection.\")\n",
    "\n",
    "        if display_type in ['sanitized_text', 'statistics' ]:\n",
    "                \n",
    "            if display_type == 'sanitized_text':\n",
    "                with gui.output_text:\n",
    "                    print(' '.join([ t.replace(' ', '_') for t in terms ]))\n",
    "                    return\n",
    "\n",
    "            if display_type == 'statistics':\n",
    "\n",
    "                wf = nltk.FreqDist(terms)\n",
    "\n",
    "                with gui.output_text:\n",
    "\n",
    "                    df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                    print('Token count: {} Vocab count: {}'.format(wf.N(), wf.B()))\n",
    "                    display(df)\n",
    "\n",
    "                with gui.output_statistics:\n",
    "\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                    wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                    data = list(zip(*wf.most_common(25)))\n",
    "                    plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "    except Exception as ex:\n",
    "        with gui.output_text:\n",
    "            logger.error(ex)\n",
    "            \n",
    "def display_cleanup_text_gui(container, wti_index):\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    \n",
    "    logger.info('Preparing corpus statistics...')\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    document_options = gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "        \n",
    "    logger.info('...loading term substitution mappings...')\n",
    "    gpe_filename = os.path.join(config.DATA_FOLDER, 'gpe_substitutions.txt')\n",
    "    gpe_substitutions = textacy_utility.load_term_substitutions(filepath=gpe_filename)\n",
    "\n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    gui = types.SimpleNamespace(\n",
    "        position=1,\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=lw('400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "        max_doc_freq=widgets.IntSlider(description='Max doc. %', min=0, max=100, value=100, step=1, layout=widgets.Layout(width='400px')),\n",
    "        mask_gpe=widgets.ToggleButton(value=False, description='Mask GPE',  tooltip='Replace geographical entites with `_gpe_`', icon='check'),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ None, 'lemma', 'lower' ], value='lower', layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        left=widgets.Button(description='<', button_style='Success', layout=lw('40px')),\n",
    "        right=widgets.Button(description='>', button_style='Success', layout=lw('40px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    logger.info('...word counts...')\n",
    "    word_count_scores = dict(\n",
    "        lemma=textacy_utility.generate_word_count_score(corpus, 'lemma', gui.min_freq.max),\n",
    "        lower=textacy_utility.generate_word_count_score(corpus, 'lower', gui.min_freq.max),\n",
    "        orth=textacy_utility.generate_word_count_score(corpus, 'orth', gui.min_freq.max)\n",
    "    )\n",
    "    logger.info('...word document count...')\n",
    "    word_document_count_scores = dict(\n",
    "        lemma=textacy_utility.generate_word_document_count_score(corpus, 'lemma', gui.max_doc_freq.min),\n",
    "        lower=textacy_utility.generate_word_document_count_score(corpus, 'lower', gui.max_doc_freq.min),\n",
    "        orth=textacy_utility.generate_word_document_count_score(corpus, 'orth', gui.max_doc_freq.min)\n",
    "    )\n",
    "\n",
    "    logger.info('...done!')\n",
    "    opts = dict(\n",
    "        min_freq=gui.min_freq,\n",
    "        max_doc_freq=gui.max_doc_freq,\n",
    "        mask_gpe=gui.mask_gpe,\n",
    "        ngrams=gui.ngrams,\n",
    "        min_word=gui.min_word,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        named_entities=gui.named_entities,\n",
    "        include_pos=gui.include_pos\n",
    "    )\n",
    "    uix = widgets.interactive(\n",
    "        display_cleaned_up_text,\n",
    "        container=widgets.fixed(container),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        treaty_id=gui.document_id,\n",
    "        gpe_substitutions=widgets.fixed(gpe_substitutions),\n",
    "        word_count_scores=widgets.fixed(word_count_scores),\n",
    "        word_document_count_scores=widgets.fixed(word_document_count_scores),\n",
    "        **opts\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([gui.document_id]),\n",
    "                widgets.HBox([gui.left, gui.right], layout=widgets.Layout(align_items='flex-end')),\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq,\n",
    "                gui.max_doc_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.mask_gpe,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "    \n",
    "    def back_handler(*args):\n",
    "        if gui.position == 0:\n",
    "            return\n",
    "        #gui.output.clear_output()\n",
    "        gui.position = (gui.position - 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "        #itw.update()\n",
    "        \n",
    "    def forward_handler(*args):\n",
    "        #gui.output.clear_output()\n",
    "        gui.position = (gui.position + 1) % len(document_options)\n",
    "        gui.document_id.value = document_options[gui.position][1]\n",
    "    \n",
    "    gui.left.on_click(back_handler)\n",
    "    gui.right.on_click(forward_handler)\n",
    "    \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "try:\n",
    "    xgui, xuix = display_cleanup_text_gui(current_corpus_container(), treaty_repository.current_wti_index())\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Most Discriminating Terms<span style='color: blue; float: right'>OPTIONAL</span>\n",
    "References\n",
    "King, Gary, Patrick Lam, and Margaret Roberts. “Computer-Assisted Keyword and Document Set Discovery from Unstructured Text.” (2014). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.1445&rep=rep1&type=pdf.\n",
    "\n",
    "Displays the *most discriminating words* between two sets of treaties. Each treaty group can be filtered by country and period (signed year). In this way, the same group of countries can be studied for different time periods, or different groups of countries can be studied for the same time period. If \"Closed region\" is checked then **both** parties must be to the selected set of countries, from each region. In this way, one can for instance compare treaties signed between countries within the WTI group \"Communists\", against treaties signed within \"Western Europe\". \n",
    "\n",
    "<b>#terms</b> The number of most discriminating terms to return for each group.<br>\n",
    "<b>#top</b> Only terms with a frequency within the top #top terms out of all terms<br>\n",
    "<b>Closed region</b> If checked, then <u>both</u> treaty parties must be within selected region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import most_discriminating_terms_gui\n",
    "try:\n",
    "    most_discriminating_terms_gui.display_gui(treaty_repository.current_wti_index(), current_corpus())\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and export region vs region MDT files as Excel Spreadsheets\n",
    "\n",
    "import spacy\n",
    "\n",
    "def create_mdt(group1, group2, include_pos, closed_region):  \n",
    "    most_discriminating_terms_gui.compute_most_discriminating_terms(\n",
    "        treaty_repository.current_wti_index(),\n",
    "        current_corpus(),\n",
    "        group1=config.get_region_parties(*group1),\n",
    "        group2=config.get_region_parties(*group2),\n",
    "        top_n_terms=100,\n",
    "        max_n_terms=2000,\n",
    "        include_pos=include_pos,\n",
    "        period1=(1945, 1972),\n",
    "        period2=(1945, 1972),\n",
    "        closed_region=closed_region,\n",
    "        normalize=spacy.attrs.LEMMA,\n",
    "        output_filename = os.path.join(config.DATA_FOLDER,\n",
    "            'MDT_{}_vs_{}_({})_{}.xlsx'.format(\n",
    "                '+'.join(['R{}'.format(x) for x in group1]),\n",
    "                '+'.join(['R{}'.format(x) for x in group2]),\n",
    "                ','.join(include_pos),\n",
    "                'CLOSED' if closed_region else 'OPEN')\n",
    "            )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_pos=['ADJ', 'VERB', 'NOUN']\n",
    "\n",
    "create_mdt((1,), (2,3), include_pos, True)\n",
    "create_mdt((2,), (1,3), include_pos, True)\n",
    "create_mdt((3,), (1,2), include_pos, True)\n",
    "create_mdt((1,), (2,3), include_pos, False)\n",
    "create_mdt((2,), (1,3), include_pos, False)\n",
    "create_mdt((3,), (1,2), include_pos, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> Corpus Statistics<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> List of Most Frequent Words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_frequencies_gui               \n",
    "try:\n",
    "    word_frequencies_gui.word_frequency_gui(treaty_repository.current_wti_index(), current_corpus())\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> Corpus and Document Sizes<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import types\n",
    "from common import domain_logic\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def compute_corpus_statistics(\n",
    "    data_folder,\n",
    "    wti_index,\n",
    "    container,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    parties=None,\n",
    "    target='lemma',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):\n",
    "    corpus = container.textacy_corpus\n",
    "\n",
    "    value_columns = list(textacy_utility.POS_NAMES) if (len(include_pos or [])) == 0 else list(include_pos)\n",
    "        \n",
    "    documents = domain_logic.get_corpus_documents(corpus)\n",
    "    \n",
    "    if len(parties or []) > 0:\n",
    "        documents = documents[documents.party1.isin(parties)|documents.party2.isin(parties)]\n",
    "\n",
    "    documents['signed_lustrum'] = (documents.signed_year - documents.signed_year.mod(5)).astype(int) \n",
    "    documents['signed_decade'] = (documents.signed_year - documents.signed_year.mod(10)).astype(int)\n",
    "    documents['total'] = documents[value_columns].apply(sum, axis=1)\n",
    "\n",
    "    #documents = documents.groupby(group_by_column).agg(sum) #.reset_index()\n",
    "    aggregates = { x: ['sum'] for x in value_columns }\n",
    "    aggregates['total'] = ['sum', 'mean', 'min', 'max', 'size' ]\n",
    "    #if group_by_column != 'treaty_id':\n",
    "    documents = documents.groupby(group_by_column).agg(aggregates)\n",
    "    documents.columns = [ ('Total, ' + x[1].lower()) if x[0] == 'total' else x[0] for x in documents.columns ]\n",
    "    columns = sorted(value_columns) + sorted([ x for x in documents.columns if x.startswith('Total')])\n",
    "    return documents[columns]\n",
    "        \n",
    "def corpus_statistics_gui(data_folder, wti_index, container, compute_callback, display_callback):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    include_pos_tags =  list(textacy_utility.POS_NAMES)\n",
    "    pos_options = include_pos_tags\n",
    "        \n",
    "    counter = collections.Counter(corpus.word_counts(normalize='lemma', weighting='count', as_strings=True))\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100) ]\n",
    "\n",
    "    treaty_time_groupings = wti_index.get_treaty_time_groupings()\n",
    "    group_by_options = { treaty_time_groupings[k]['title']: k for k in treaty_time_groupings }\n",
    "    # output_type_options = [ ( 'Table', 'table' ), ( 'Pivot', 'pivot' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('180px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        target=widgets.Dropdown(description='Normalize', options={ '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }, value='lemma', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list([]), rows=7, layout=widgets.Layout(width='180px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=lw('200px')),\n",
    "        #output_type=widgets.Dropdown(description='Output', value='table', options=output_type_options, layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute', layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.group_by_column,\n",
    "                #gui.output_type,\n",
    "                gui.party_preset,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties,\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "\n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df_freqs = compute_callback(\n",
    "                data_folder=data_folder,\n",
    "                wti_index=wti_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                target=gui.target.value,\n",
    "                group_by_column=gui.group_by_column.value,\n",
    "                parties=gui.parties.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "            )\n",
    "            display_callback(gui, df_freqs)\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "\n",
    "def plot_simple(xs, ys, **figopts):\n",
    "    source = bokeh.models.ColumnDataSource(dict(x=xs, y=ys))\n",
    "    figopts = utility.extend(dict(title='', toolbar_location=\"right\"), figopts)\n",
    "    p = bokeh.plotting.figure(**figopts)\n",
    "    glyph = p.line(source=source, x='x', y='y', line_color=\"#b3de69\")\n",
    "    return p\n",
    "\n",
    "def display_corpus_statistics(gui, df):\n",
    "    display(df)\n",
    "    #with gui.output:\n",
    "    #    plotopts=dict(plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "    #    p = plot_simple(X.index, X['Total, mean'], **plotopts)\n",
    "    #    bokeh.plotting.show(p)\n",
    "\n",
    "   \n",
    "try:\n",
    "    gui = corpus_statistics_gui(config.DATA_FOLDER, treaty_repository.current_wti_index(), current_corpus_container(), compute_callback=compute_corpus_statistics, display_callback=display_corpus_statistics)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
