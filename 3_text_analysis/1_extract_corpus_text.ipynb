{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tCoIR - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "#import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import text_corpus\n",
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n",
    "#current_document_index = lambda: current_corpus_container().document_index\n",
    "\n",
    "#import domain_logic_vatican as domain_logic\n",
    "from domain_logic_config import current_domain as domain_logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(domain_logic.DATA_FOLDER, container=container, domain_logic=domain_logic)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Extract Text From Corpus <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "#import domain_logic_vatican as domain_logic\n",
    "from domain_logic_config import current_domain as domain_logic\n",
    "\n",
    "DF_TAGSET = pd.read_csv(os.path.join(domain_logic.DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "\n",
    "def chunks(l, n):\n",
    "    '''Returns list l in n-sized chunks'''\n",
    "    if (n or 0) == 0:\n",
    "        yield l\n",
    "    else:\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "def tokenize_docs(docs, **opts): \n",
    "    try:\n",
    "        document_id = 0\n",
    "        normalize = opts['normalize'] or 'orth'\n",
    "        term_substitutions = opts.get('substitutions', {})\n",
    "        word_counts = opts.get('word_counts', {})\n",
    "        word_document_counts = opts.get('word_document_counts', {})\n",
    "        extra_stop_words = set([])\n",
    "\n",
    "        if opts['min_freq'] > 1:\n",
    "            stop_words = utility.extract_counter_items_within_threshold(word_counts[normalize], 1, opts['min_freq'])\n",
    "            extra_stop_words.update(stop_words)\n",
    "\n",
    "        if opts['max_doc_freq'] < 100:\n",
    "            stop_words = utility.extract_counter_items_within_threshold(word_document_counts[normalize], opts['max_doc_freq'], 100)\n",
    "            extra_stop_words.update(stop_words)\n",
    "\n",
    "        extract_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=opts['ngrams'],\n",
    "                named_entities=opts['named_entities'],\n",
    "                normalize=opts['normalize'],\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                min_freq=opts['min_freq'],\n",
    "                include_pos=opts['include_pos'],\n",
    "                filter_stops=opts['filter_stops'],\n",
    "                filter_punct=opts['filter_punct']\n",
    "            ),\n",
    "            extra_stop_words=extra_stop_words,\n",
    "            substitutions=(term_substitutions if opts.get('substitute_terms', False) else None),\n",
    "        )\n",
    "\n",
    "        for document_name, doc in docs:\n",
    "            print(document_name)\n",
    "\n",
    "            terms = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n",
    "            \n",
    "            chunk_size = opts.get('chunk_size', 0)\n",
    "            chunk_index = 0\n",
    "            for tokens in chunks(terms, chunk_size):\n",
    "                yield document_id, document_name, chunk_index, tokens\n",
    "                chunk_index += 1\n",
    "\n",
    "            document_id += 1\n",
    "                    \n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "        \n",
    "def store_tokenized_corpus(tokenized_docs, corpus_source_filepath, **opts): \n",
    "    \n",
    "    filepath = utility.path_add_timestamp(corpus_source_filepath)\n",
    "    filepath = utility.path_add_suffix(filepath, '.tokenized')\n",
    "    \n",
    "    file_stats = []\n",
    "    process_count = 0\n",
    "    \n",
    "    # TODO: Enable store of all documents line-by-line in a single file\n",
    "    with zipfile.ZipFile(filepath, \"w\") as zf:\n",
    "        \n",
    "        for document_id, document_name, chunk_index, tokens in tokenized_docs: \n",
    "            \n",
    "            text = ' '.join([ t.replace(' ', '_') for t in tokens ])\n",
    "            store_name  = utility.path_add_sequence(document_name, chunk_index, 4)\n",
    "            \n",
    "            zf.writestr(store_name, text, zipfile.ZIP_DEFLATED)\n",
    "            \n",
    "            file_stats.append((document_id, document_name, chunk_index, len(tokens)))\n",
    "            \n",
    "            if process_count % 100 == 0:\n",
    "                logger.info('Stored {} files...'.format(process_count))\n",
    "                \n",
    "            process_count += 1\n",
    "            \n",
    "            \n",
    "    df_summary = pd.DataFrame(file_stats, columns=['document_id', 'document_name', 'chunk_index', 'n_tokens'])\n",
    "    \n",
    "    return filepath, df_summary\n",
    "\n",
    "def display_generate_tokenized_corpus_gui(corpus, corpus_source_filepath, subst_filename=None):\n",
    "    \n",
    "    filenames = [ doc.metadata['filename'] for doc in corpus ]\n",
    "    document_index = domain_logic.compile_documents_by_filename(filenames)\n",
    "    term_substitutions = { }\n",
    "    \n",
    "    if subst_filename is not None:\n",
    "        logger.info('Loading term substitution mappings...')\n",
    "        term_substitutions = textacy_utility.load_term_substitutions(subst_filename, default_term='_masked_', delim=';', vocab=corpus.spacy_vocab)\n",
    "        \n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = [('(All)', None)] + sorted([(k + ' (' + v + ')', k) for k,v in pos_tags.items() ])\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    \n",
    "    lw = lambda width: widgets.Layout(width=width)\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1, layout=widgets.Layout(width='400px')),\n",
    "        max_doc_freq=widgets.IntSlider(description='Min doc. %', min=75, max=100, value=100, step=1, layout=widgets.Layout(width='400px')),\n",
    "        substitute_terms=widgets.ToggleButton(value=False, description='Mask GPE',  tooltip='Replace geographical entites with `_gpe_`', icon='check'),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        chunk_size=widgets.Dropdown(description='Chunk size', options=[('None', 0), ('500', 500), ('1000', 1000), ('2000', 2000) ], value=0, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ None, 'lemma', 'lower' ], value='lower', layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('100px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "    )\n",
    "    \n",
    "    logger.info('Preparing corpus statistics...')\n",
    "    logger.info('...word counts...')\n",
    "    word_counts = { k: textacy_utility.generate_word_count_score(corpus, k, gui.min_freq.max) for k in [ 'lemma', 'lower', 'orth' ] }\n",
    "    \n",
    "    logger.info('...word document count...')\n",
    "    word_document_counts = { k: textacy_utility.generate_word_document_count_score(corpus, k, gui.max_doc_freq.min) for k in [ 'lemma', 'lower', 'orth' ] }\n",
    "\n",
    "    logger.info('...done!')\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([gui.normalize, gui.chunk_size]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq,\n",
    "                gui.max_doc_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.substitute_terms,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "    \n",
    "    def compute_callback(*_args):\n",
    "        gui.compute.disabled = True\n",
    "        filepath = ''\n",
    "        opts = dict(\n",
    "            min_freq=gui.min_freq.value,\n",
    "            max_doc_freq=gui.max_doc_freq.value,\n",
    "            substitute_terms=gui.substitute_terms.value,\n",
    "            ngrams=gui.ngrams.value,\n",
    "            min_word=gui.min_word.value,\n",
    "            normalize=gui.normalize.value,\n",
    "            filter_stops=gui.filter_stops.value,\n",
    "            filter_punct=gui.filter_punct.value,\n",
    "            named_entities=gui.named_entities.value,\n",
    "            include_pos=gui.include_pos.value,\n",
    "            chunk_size=gui.chunk_size.value,\n",
    "            term_substitutions=term_substitutions,\n",
    "            word_counts=word_counts,\n",
    "            word_document_counts=word_document_counts\n",
    "        )\n",
    "        \n",
    "        with gui.output:\n",
    "\n",
    "            docs = ((doc.metadata['filename'], doc) for doc in corpus)\n",
    "            \n",
    "            tokenized_docs = tokenize_docs(docs, **opts)\n",
    "            \n",
    "            filepath, df_summary = store_tokenized_corpus(tokenized_docs, corpus_source_filepath, **opts)\n",
    "            \n",
    "        gui.output.clear_output()\n",
    "        \n",
    "        with gui.output:\n",
    "            logger.info('Process DONE!')\n",
    "            logger.info(\"Result stored in '{}'\".format(filepath))\n",
    "            display(df_summary)\n",
    "            \n",
    "        gui.compute.disabled = False\n",
    "        \n",
    "    gui.compute.on_click(compute_callback)\n",
    "    \n",
    "    return gui\n",
    "\n",
    "try:\n",
    "    subst_filename = os.path.join(domain_logic.DATA_FOLDER, 'term_substitutions.txt')\n",
    "    corpus = current_corpus_container().textacy_corpus\n",
    "    corpus_path =  current_corpus_container().prepped_source_path\n",
    "    #if not corpus is None :\n",
    "    #    docs = textacy_corpus_document_stream(corpus)\n",
    "    if corpus is None:\n",
    "        logger.info('Please load corpus!')\n",
    "    else:\n",
    "        display_generate_tokenized_corpus_gui(corpus, corpus_path)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
