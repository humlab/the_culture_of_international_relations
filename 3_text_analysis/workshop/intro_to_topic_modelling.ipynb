{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started\n",
    "- Open Url https://open-science.humlab.umu.se<br>\n",
    "- Username **phd_user_01, phd_user_02, ..., phd_user_11**<br>\n",
    "- Password **phd_course_2018**<br>\n",
    "- Open folder **phd_course** (just click on it) and notebook **intro_to_topic_modelling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Jupyter Notebook?\n",
    "[Jupyter](http://jupyter.org/) is an open-source software for **interactive and reproducible computing**.<br>\n",
    "<span style=\"float:left\"><img src=\"./images/narrative_new.svg\" style=\"width: 300px;padding: 0; margin: 0;\"></span>\n",
    "<br>\n",
    "> - The **open science movement** is a driving force for Jupyter's popularity.<br>\n",
    "> - Which in part is a response to the **reproducibility crisis in science** and the **statistical crisis in science**<br>\n",
    "> - Jupyter Notebooks contain **excutable code, equations, visualizations and narrative text**.<br>\n",
    "> - It is a **web application** with a simple and easy to use web interface.\n",
    "> - Supports a large number of programming languages (50+ e.g. Python, R, JavaScript)\n",
    "> - Sponsered by large companies such as Google and Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Instructions on How to Use Notebooks\n",
    "- **Menu Help** -> User Interface Tour\n",
    "- **<span style=\"color: red\">CODE CELLS**</span> contain Python script code and have **In [x]** in the left margin\n",
    "  - **In []** indicates that the code cell hasn't been executed yet\n",
    "  - **In [n]** indicates that the code has been executed(n is an integer)\n",
    "  - **In [\\*]** indicates that the code is executing, or waiting to be executed (i.e. other cells are executing)\n",
    "  - **Out[n]** indicates the output (or result) of a cell's execution and is directly below the executed cell\n",
    "- **The current code** is highlighted with a blue border - you make it current by clicking on it\n",
    "- **<span style=\"color: red\">SHIFT+ENTER</span>** or **PLAY BUTTON** executes the current cell. Code cells aren't executed automatically\n",
    "- **SHIFT+ENTER** automatically selects the next code cell\n",
    "- **SHIFT+ENTER** can hence be used repeatedly to executes the code cells in sequence\n",
    "- **Menu Cell -> Run All** executes the entire notebook in a single step (notice how \"In [\\*]\" indicators change to \"In [n]\")\n",
    "- **Double-Click** on a cell to edit its content.\n",
    "- **ESC key** Leaves edit mode (or just click on any other cell).\n",
    "- **Kernel -> Restart** restarts server side kernel (use if notebook seems stuck)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Topic Modelling?\n",
    "\n",
    "Topic modelling (TM) can be thought of as:\n",
    "- A method for finding **groups of words** that in some way **capture the information** in a collection of documents\n",
    "- A form of **text classification** - a way to cluster documents\n",
    "- A form of **text mining** - a way to **find recurring patterns of words** in a collection\n",
    "- TM assumes that documents have these kinds of **underlying patterns**\n",
    "- This means that certain **\"groups of words\" (i.e. topics) occur more frequently** in a specific document\n",
    "- These groups (or patterns) of words are the **topics**\n",
    "\n",
    "**<span style=\"color: red\">It is up to YOU to determine the \"meaning\" of the topics!</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an LDA Topic Model?\n",
    "- LDA is a so called \"generative probabilistic\" model\n",
    "- The premise of LDA is the assumption that the documents at hand **have been generated via a statistical model/process**\n",
    "- Each topic is in this model a simple **a word frequency distribution**\n",
    "- A simplified view of how a document is generated is\n",
    "  - Select the document's mix of topics (e.g. *Topic X* 40%, *Topic Y* 25%, *Topic Z* 35%)\n",
    "  - Generate the document's words by repeating\n",
    "    - Draw a topic from the topic distribution\n",
    "    - Draw a word from that topic's word distribution\n",
    "\n",
    "**<span style=\"color: black\">Given this imaginary generative process, the corpus at hand is the correct answer!</span>**<br>\n",
    "Commonly used computational processes can be used to fit the corpus to the statistical model, which gives the topic distributions.<br>\n",
    "See *(Blei, 2003: Latent dirichlet allocation* [PDF](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<span style=\"float:left\">\n",
    "    <img src=\"./images/blei_lda.jpg\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<span style=\"float:left\">\n",
    "    <img src=\"./images/blei_2012b.png\" style=\"width: 600px;padding: 0; margin: 0;\">\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenges\n",
    "- **Whatâ€™s easy for humans can be extremely hard for computers**\n",
    "  - **Ambiguity** and fuzziness of terms and phrases\n",
    "  - Poor **data quality**, errors in data, wrong data, missing data, ambigious data\n",
    "  - Understand **domain contexts**, metadata, domain-specific data\n",
    "  - **Data size** (to much, to little)\n",
    "  - How to understand **internal representations** of data used by computational methods\n",
    "  - Internal representations are **simplified views** of the actual data (e.g. \"bag-of-words\" model)\n",
    "  - How to verify **performance** (correctness of result)<br>\n",
    "  - etc...\n",
    "\n",
    "**Human-in-the-loop or supervised learning can be very expensive**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Risks\n",
    "- Using tools and methods **without fully understanding** them\n",
    "- Using tools and methods **for non-intended purposes or in new contexts**\n",
    "- Risk of **data dredging**, p-hacking, \"the statistical crisis\".\n",
    "- The risk that **engineer makes micro-decisions** the researcher don't know about, or don't fully understand.\n",
    "- The risk of **reading to much into visualizations** (networks, layouts, clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Sample High-Level Text Analysis Workflow\n",
    "\n",
    "<img src=\"./images/text_analysis_workflow.svg\" alt=\"\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: blue'>IDENTIFY</span> 25 Academic Papers <span style='color: blue; float: right'></span>  \n",
    "The source data consists of 25 english academic articles downloded as PDF from Zotera.\n",
    "## <span style='color: green'>COLLECT</span> Extract Text From PDFs <span style='color: blue; float: right'>SKIP</span>  \n",
    "The first step is to extract the text from the PDF files. This can also be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed, PDFPage\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "\n",
    "def extract_pdf_text(filename):\n",
    "    text_lines = []\n",
    "    with open(filename, 'rb') as fp:\n",
    "        \n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "\n",
    "        resource_manager = PDFResourceManager()\n",
    "\n",
    "        result_buffer = StringIO()\n",
    "\n",
    "        device = TextConverter(resource_manager, result_buffer, codec='utf-8', laparams=LAParams())\n",
    "\n",
    "        interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        lines = result_buffer.getvalue().splitlines()\n",
    "        for line in lines:\n",
    "            text_lines.append(line)\n",
    "\n",
    "    return text_lines\n",
    "\n",
    "\n",
    "def extract_pdf_texts(source_folder, target_zip_filename):\n",
    "    with zipfile.ZipFile(target_zip_filename, 'w', zipfile.ZIP_DEFLATED) as target_zip:\n",
    "\n",
    "        for filename in glob.glob(os.path.join(source_folder,'*.pdf')):\n",
    "\n",
    "            print('Processing: ' + filename)\n",
    "\n",
    "            text_lines = extract_pdf_text(filename)\n",
    "\n",
    "            target_filename = os.path.splitext(os.path.split(filename)[1])[0] + '.txt'\n",
    "            target_filename = target_filename.lower().replace(' ', '_').replace(',','')\n",
    "\n",
    "            target_zip.writestr(target_filename, '\\n'.join(text_lines))\n",
    "            \n",
    "#source_folder = './data/pdf'\n",
    "#target_zip_filename = 'data/paper_extracted_texts.zip'\n",
    "#extract_pdf_texts(source_folder, target_zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>INITIALIZE </span> Setup and Initialize the Notebook <span style='color: red; float: right'>MANDATORY RUN!</span>  \n",
    "The following CODE CELL must be run once to set up the run time environment. Please select the cell and hit **SHIFT-ENTER** or the **RUN** button in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# folded code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, warnings, types, sys\n",
    "import numpy as np, pandas as pd\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import re, string, zipfile\n",
    "import nltk, spacy, textacy, textacy.extract, textacy.preprocess\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "from gensim import corpora, models, matutils\n",
    "from IPython.display import display, HTML #, clear_output, IFrame\n",
    "from pivottablejs import pivot_ui\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "logger = utility.getLogger() #format=\"%(levelname)s;%(message)s\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "pd.set_option('precision', 10)\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "\n",
    "DEFAULT_TERM_PARAMS = dict(\n",
    "    args=dict(ngrams=1, named_entities=True, normalize='lemma', as_strings=True),\n",
    "    kwargs=dict(filter_stops=True, filter_punct=True, filter_nums=True, min_freq=1, drop_determiners=True, include_pos=('NOUN', 'PROPN', ))\n",
    ")\n",
    "\n",
    "FIXED_STOPWORDS = ['', '\\n', 'et', 'al', 'et.al.' ]\n",
    "def filter_terms(doc, term_args, chunk_size=None, min_length=2):\n",
    "    kwargs = utility.extend({}, DEFAULT_TERM_PARAMS['kwargs'], term_args['kwargs'])\n",
    "    args = utility.extend({}, DEFAULT_TERM_PARAMS['args'], term_args['args'])\n",
    "    terms = (x for x in doc.to_terms_list(\n",
    "        args['ngrams'],\n",
    "        args['named_entities'],\n",
    "        args['normalize'],\n",
    "        args['as_strings'],\n",
    "        **kwargs\n",
    "    ) if len(x) >= min_length and x not in FIXED_STOPWORDS)\n",
    "    return terms\n",
    "        \n",
    "def slim_title(x):\n",
    "    try:\n",
    "        m = re.match('.*\\((.*)\\)$', x).groups()\n",
    "        if m is not None and len(m) > 0:\n",
    "            return m[0]\n",
    "        return ' '.join(x.split(' ')[:3]) + '...'\n",
    "    except:\n",
    "        return x\n",
    "            \n",
    "LANGUAGE = 'en'\n",
    "SOURCE_FOLDER = '../data'\n",
    "\n",
    "EXTRACTED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'dummy.zip')\n",
    "EDITED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'dummy.zip')\n",
    "PREPROCESSED_TEXT_FILENAME = os.path.join(SOURCE_FOLDER, 'treaty_text_corpora_20181115_preprocessed.zip')\n",
    "\n",
    "SOURCE_FILES = {\n",
    "    'source_text_raw': { 'filename': EXTRACTED_TEXT_FILENAME, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "    'source_text_edited': { 'filename': EDITED_TEXT_FILENAME, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "    'source_text_preprocessed': { 'filename': PREPROCESSED_TEXT_FILENAME, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "}\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "AGGREGATES = { 'mean': np.mean, 'sum': np.sum, 'max': np.max, 'std': np.std }\n",
    "\n",
    "logger.info('POS tag set: ' + ' '.join(list(DF_TAGSET.POS.unique())))\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg') #'pdf', 'svg')\n",
    "    \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute an LDA Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE: </span> Load and Prepare the Text Corpus <span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "def preprocess_text(source_filename, target_filename=None):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    basename, extension = os.path.splitext(source_filename)\n",
    "    target_filename = target_filename or basename + '_preprocessed' + extension\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            logger.info('Processing ' + filename)\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            text = textacy.preprocess.replace_urls(text)\n",
    "            text = textacy.preprocess.replace_emails(text)\n",
    "            text = textacy.preprocess.replace_phone_numbers(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            zf.writestr(filename, text)\n",
    "            \n",
    "def create_textacy_corpus(source_filename, language, preprocess_args):\n",
    "    make_title = lambda filename: filename.replace('_', ' ').replace('.txt', '').title()\n",
    "    filenames = get_filenames(source_filename)\n",
    "    corpus = textacy.Corpus(language)\n",
    "    text_stream = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    for filename, text in text_stream:\n",
    "        logger.info('Processing ' + filename)\n",
    "        text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "        text = textacy.preprocess.preprocess_text(text, **preprocess_args)\n",
    "        corpus.add_text(text, dict(filename=filename, title=make_title(filename)))\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    return corpus\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "    return doc\n",
    "\n",
    "def generate_textacy_corpus(source_filename, language, corpus_args, preprocess_args, merge_named_entities=True, force=False):\n",
    "    \n",
    "    corpus_tag = '_'.join([ k for k in preprocess_args if preprocess_args[k] ]) + \\\n",
    "        '_disable(' + ','.join(corpus_args.get('disable', [])) +')'\n",
    "    \n",
    "    textacy_corpus_filename = os.path.join(SOURCE_FOLDER, 'corpus_{}_{}.pkl'.format(language, corpus_tag))\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    logger.info('Loading model: english...')\n",
    "    nlp = textacy.load_spacy('en_core_web_sm', **corpus_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "        \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "\n",
    "    if force or not os.path.isfile(textacy_corpus_filename):\n",
    "        logger.info('Working: Computing new corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = create_textacy_corpus(source_filename, nlp, preprocess_args)\n",
    "        corpus.save(textacy_corpus_filename)\n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + textacy_corpus_filename + '...')\n",
    "        corpus = textacy.Corpus.load(textacy_corpus_filename)\n",
    "        \n",
    "    if merge_named_entities:\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Note: named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    return textacy_corpus_filename, corpus\n",
    "\n",
    "def assign_document_titles(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['title']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df = pd.DataFrame([\n",
    "        (document_id, doc.metadata['title'], doc.metadata['filename'])\n",
    "                for document_id, doc in enumerate(corpus) ], columns=['document_id', 'title', 'filename']\n",
    "    ).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "#if not os.path.isfile(PREPROCESSED_TEXT_FILENAME):\n",
    "#    logger.info(\"Preprocessing text archive...\")\n",
    "#    preprocess_text(EDITED_TEXT_FILENAME, PREPROCESSED_TEXT_FILENAME)\n",
    "    \n",
    "TEXTACY_CORPUS_FILENAME, CORPUS = generate_textacy_corpus(PREPROCESSED_TEXT_FILENAME, LANGUAGE, corpus_args=dict(), preprocess_args=dict(), merge_named_entities=True, force=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "def display_cleanup_text_gui(corpus, callback):\n",
    "    \n",
    "    documents = get_corpus_documents(corpus)\n",
    "    document_options = {v: k for k, v in documents['title'].to_dict().items()}\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=[1,2,3], value=1, layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        callback,\n",
    "\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        document_id=gui.document_id,\n",
    "        \n",
    "        ngrams=gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.document_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(corpus, gui, display_type, document_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    #Additional candidates;\n",
    "    #is_alpha\tbool\tDoes the token consist of alphabetic characters? Equivalent to token.text.isalpha().\n",
    "    #is_ascii\tbool\tDoes the token consist of ASCII characters? Equivalent to [any(ord(c) >= 128 for c in token.text)].\n",
    "    #like_url\tbool\tDoes the token resemble a URL?\n",
    "    #like_email\tbool\tDoes the token resemble an email address?\n",
    "\n",
    "    doc = corpus[document_id]\n",
    "    \n",
    "    terms = [ x for x in doc.to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_filename = SOURCE_FILES[display_type]['filename']\n",
    "        description =  SOURCE_FILES[display_type]['description']\n",
    "        text = get_text(source_filename, doc.metadata['filename'])\n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "\n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join(list(terms)))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                print('Word count (number of terms): {}'.format(wf.N()))\n",
    "                print('Unique word count (vocabulary): {}'.format(wf.B()))\n",
    "                print(' ')\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                display(df)\n",
    "\n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "xgui, xuix = display_cleanup_text_gui(CORPUS, display_cleaned_up_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute an LDA Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "class LdaDataCompiler():\n",
    "    \n",
    "    @staticmethod\n",
    "    def compile_dictionary(model):\n",
    "        logger.info('Compiling dictionary...')\n",
    "        token_ids, tokens = list(zip(*model.id2word.items()))\n",
    "        dfs = model.id2word.dfs.values() if model.id2word.dfs is not None else [0] * len(tokens)\n",
    "        dictionary = pd.DataFrame({\n",
    "            'token_id': token_ids,\n",
    "            'token': tokens,\n",
    "            'dfs': list(dfs)\n",
    "        }).set_index('token_id')[['token', 'dfs']]\n",
    "        return dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_weights(tm, dictionary, num_words=200):\n",
    "        logger.info('Compiling topic-tokens weights...')\n",
    "\n",
    "        df_topic_weights = pd.DataFrame(\n",
    "            [ (topic_id, token, weight)\n",
    "                for topic_id, tokens in (tm.show_topics(tm.num_topics, num_words=num_words, formatted=False))\n",
    "                    for token, weight in tokens if weight > 0.0 ],\n",
    "            columns=['topic_id', 'token', 'weight']\n",
    "        )\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_topic_weights.set_index('token'),\n",
    "            dictionary.reset_index().set_index('token'),\n",
    "            how='inner',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        return df.reset_index()[['topic_id', 'token_id', 'token', 'weight']]\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_overview(topic_token_weights, alpha, n_words=200):\n",
    "        \"\"\"\n",
    "        Group by topic_id and concatenate n_words words within group sorted by weight descending.\n",
    "        There must be a better way of doing this...\n",
    "        \"\"\"\n",
    "        logger.info('Compiling topic-tokens overview...')\n",
    "\n",
    "        df = topic_token_weights.groupby('topic_id')\\\n",
    "            .apply(lambda x: sorted(list(zip(x[\"token\"], x[\"weight\"])), key=lambda z: z[1], reverse=True))\\\n",
    "            .apply(lambda x: ' '.join([z[0] for z in x][:n_words])).reset_index()\n",
    "        df['alpha'] = df.topic_id.apply(lambda topic_id: alpha[topic_id])\n",
    "        df.columns = ['topic_id', 'tokens', 'alpha']\n",
    "\n",
    "        return df.set_index('topic_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_document_topics(model, corpus, documents, minimum_probability=0.001):\n",
    "\n",
    "        def document_topics_iter(model, corpus, minimum_probability):\n",
    "\n",
    "            data_iter = model.get_document_topics(corpus, minimum_probability=minimum_probability)\\\n",
    "                if hasattr(model, 'get_document_topics')\\\n",
    "                else model.load_document_topics()\n",
    "\n",
    "            for i, topics in enumerate(data_iter):\n",
    "                for x in topics:\n",
    "                    yield (i, x[0], x[1])\n",
    "        '''\n",
    "        Get document topic weights for all documents in corpus\n",
    "        Note!  minimum_probability=None filters less probable topics, set to 0 to retrieve all topcs\n",
    "\n",
    "        If gensim model then use 'get_document_topics', else 'load_document_topics' for mallet model\n",
    "        '''\n",
    "        logger.info('Compiling document topics...')\n",
    "        logger.info('  Creating data iterator...')\n",
    "        data = document_topics_iter(model, corpus, minimum_probability)\n",
    "        logger.info('  Creating frame from iterator...')\n",
    "        df_doc_topics = pd.DataFrame(data, columns=[ 'document_id', 'topic_id', 'weight' ]).set_index('document_id')\n",
    "        logger.info('  Merging data...')\n",
    "        df = pd.merge(documents, df_doc_topics, how='inner', left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_compiled_data(model, corpus, id2term, documents):\n",
    "\n",
    "        dictionary = LdaDataCompiler.compile_dictionary(model)\n",
    "        topic_token_weights = LdaDataCompiler.compile_topic_token_weights(model, dictionary, num_words=200)\n",
    "        topic_token_overview = LdaDataCompiler.compile_topic_token_overview(topic_token_weights, model.alpha)\n",
    "        document_topic_weights = LdaDataCompiler.compile_document_topics(model, corpus, documents, minimum_probability=0.001)\n",
    "\n",
    "        return types.SimpleNamespace(\n",
    "            dictionary=dictionary,\n",
    "            documents=documents,\n",
    "            topic_token_weights=topic_token_weights,\n",
    "            topic_token_overview=topic_token_overview,\n",
    "            document_topic_weights=document_topic_weights\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_topic_titles(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id==topic_id)]\n",
    "        df = df_temp\\\n",
    "                .sort_values('weight', ascending=False)\\\n",
    "                .groupby('topic_id')\\\n",
    "                .apply(lambda x: ' '.join(x.token[:n_words].str.title()))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_title(topic_token_weights, topic_id, n_words=100):\n",
    "        return LdaDataCompiler.get_topic_titles(topic_token_weights, topic_id, n_words=n_words).iloc[0]\n",
    "\n",
    "    #get_topics_tokens_as_text = get_topic_titles\n",
    "    #get_topic_tokens_as_text = get_topic_title\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_tokens(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id == topic_id)]\n",
    "        df = df_temp.sort_values('weight', ascending=False)[:n_words]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lda_topics(model, n_tokens=20):\n",
    "        return pd.DataFrame({\n",
    "            'Topic#{:02d}'.format(topic_id+1) : [ word[0] for word in model.show_topic(topic_id, topn=n_tokens) ]\n",
    "                for topic_id in range(model.num_topics)\n",
    "        })\n",
    "\n",
    "# OBS OBS! https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "DEFAULT_VECTORIZE_PARAMS = dict(tf_type='linear', apply_idf=False, idf_type='smooth', norm='l2', min_df=1, max_df=0.95)\n",
    "\n",
    "def compute_topic_model(corpus, tick=utility.noop, method='sklearn_lda', vec_args=None, term_args=None, tm_args=None, **args):\n",
    "    \n",
    "    tick()\n",
    "    vec_args = utility.extend({}, DEFAULT_VECTORIZE_PARAMS, vec_args)\n",
    "    \n",
    "    terms_iter = lambda: (filter_terms(doc, term_args) for doc in corpus)\n",
    "    tick()\n",
    "    \n",
    "    vectorizer = textacy.Vectorizer(**vec_args)\n",
    "    doc_term_matrix = vectorizer.fit_transform(terms_iter())\n",
    "\n",
    "    if method.startswith('sklearn'):\n",
    "        tm_model = textacy.TopicModel(method.split('_')[1], **tm_args)\n",
    "        tm_model.fit(doc_term_matrix)\n",
    "        tick()\n",
    "        doc_topic_matrix = tm_model.transform(doc_term_matrix)\n",
    "        tick()\n",
    "        tm_id2word = vectorizer.id_to_term\n",
    "        tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        compiled_data = None # FIXME\n",
    "    else:\n",
    "        doc_topic_matrix = None # ?\n",
    "        tm_id2word = corpora.Dictionary(terms_iter())\n",
    "        tm_corpus = [ tm_id2word.doc2bow(text) for text in terms_iter() ]\n",
    "        #tm_id2word = vectorizer.id_to_term\n",
    "        #tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        tm_model = models.LdaModel(\n",
    "            tm_corpus, \n",
    "            num_topics  =  tm_args.get('n_topics', 0),\n",
    "            id2word     =  tm_id2word,\n",
    "            iterations  =  tm_args.get('max_iter', 0),\n",
    "            passes      =  20,\n",
    "            alpha       = 'asymmetric'\n",
    "        )\n",
    "        documents = get_corpus_documents(corpus)\n",
    "        compiled_data = LdaDataCompiler.compute_compiled_data(tm_model, tm_corpus, tm_id2word, documents)\n",
    "    \n",
    "    tm_data = types.SimpleNamespace(\n",
    "        tm_model=tm_model,\n",
    "        tm_id2term=tm_id2word,\n",
    "        tm_corpus=tm_corpus,\n",
    "        doc_term_matrix=doc_term_matrix,\n",
    "        doc_topic_matrix=doc_topic_matrix,\n",
    "        vectorizer=vectorizer,\n",
    "        compiled_data=compiled_data\n",
    "    )\n",
    "    \n",
    "    tick(0)\n",
    "    \n",
    "    return tm_data\n",
    "\n",
    "def get_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    topic_ids = range(0,doc_topic_matrix.shape[1])\n",
    "    for document_id in range(0,doc_topic_matrix.shape[1]):\n",
    "        topic_weights = doc_topic_matrix[document_id, :]\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_weights[topic_id] >= threshold:\n",
    "                yield (document_id, topic_id, topic_weights[topic_id])\n",
    "\n",
    "def get_df_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    it = get_doc_topic_weights(doc_topic_matrix, threshold)\n",
    "    df = pd.DataFrame(list(it), columns=['document_id', 'topic_id', 'weight']).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "def display_topic_model_gui(corpus, compute_callback):\n",
    "    \n",
    "    pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]\n",
    "    # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    engine_options = { 'gensim': 'gensim' } #, 'sklearn_lda': 'sklearn_lda'}\n",
    "    normalize_options = { 'None': False, 'Use lemma': 'lemma', 'Lowercase': 'lower'}\n",
    "    ngrams_options = { '1': [1], '1, 2': [1, 2], '1,2,3': [1, 2, 3] }\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        n_topics=widgets.IntSlider(description='#topics', min=5, max=50, value=20, step=1),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1),\n",
    "        max_iter=widgets.IntSlider(description='Max iterations', min=100, max=1000, value=20, step=10),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=widgets.Layout(width='200px')),\n",
    "        filter_stops=widgets.ToggleButton(value=True, description='Remove stopword',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=True, description='Remove nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        apply_idf=widgets.ToggleButton(value=False, description='Apply IDF',  tooltip='Apply TF-IDF', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=['NOUN', 'PROPN'], rows=7, layout=widgets.Layout(width='200px')),\n",
    "        method=widgets.Dropdown(description='Engine', options=engine_options, value='gensim', layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute'),\n",
    "        boxes=None,\n",
    "        output = widgets.Output(), # layout={'height': '500px'}),\n",
    "        model=None\n",
    "    )\n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.n_topics,\n",
    "                gui.min_freq,\n",
    "                gui.max_iter\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners,\n",
    "                gui.apply_idf\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.method\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.VBox([gui.output]), # ,layout=widgets.Layout(top='20px', height='500px',width='100%'))\n",
    "    ])\n",
    "    fx = lambda *args: compute_callback(corpus, gui, *args)\n",
    "    gui.compute.on_click(fx)\n",
    "    return gui\n",
    "    \n",
    "\n",
    "def compute_callback(corpus, gui, *args):\n",
    "    \n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "    \n",
    "    tick(1)\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        vec_args = dict(apply_idf=gui.apply_idf.value)\n",
    "        term_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=gui.ngrams.value,\n",
    "                named_entities=gui.named_entities.value,\n",
    "                normalize=gui.normalize.value,\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                filter_nums=gui.filter_nums.value,\n",
    "                drop_determiners=gui.drop_determiners.value,\n",
    "                min_freq=gui.min_freq.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                filter_stops=gui.filter_stops.value,\n",
    "                filter_punct=True\n",
    "            )\n",
    "        )\n",
    "        tm_args = dict(\n",
    "            n_topics=gui.n_topics.value,\n",
    "            max_iter=gui.max_iter.value,\n",
    "            learning_method='online', \n",
    "            n_jobs=1\n",
    "        )\n",
    "        method = gui.method.value\n",
    "        gui.model = compute_topic_model(\n",
    "            corpus=corpus,\n",
    "            tick=tick,\n",
    "            method=method,\n",
    "            vec_args=vec_args,\n",
    "            term_args=term_args,\n",
    "            tm_args=tm_args\n",
    "        )\n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        #display(gui.model.compiled_data.topic_token_overview)\n",
    "        display(LdaDataCompiler.get_lda_topics(gui.model.tm_model, n_tokens=20))\n",
    "        \n",
    "TM_GUI_MODEL = display_topic_model_gui(CORPUS, compute_callback)\n",
    "display(TM_GUI_MODEL.boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_document_entities_gui(corpus):\n",
    "    \n",
    "    def display_document_entities(document_id, corpus):\n",
    "        displacy.render(corpus[document_id].spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "\n",
    "    document_widget = widgets.Dropdown(description='Paper', options={v: k for k, v in df_documents['title'].to_dict().items()}, value=0, layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(display_document_entities,document_id=document_widget, corpus=widgets.fixed(corpus))\n",
    "\n",
    "    display(widgets.VBox([document_widget, widgets.VBox([itw.children[-1]],layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    display_document_entities_gui(CORPUS)\n",
    "except Except as ex:\n",
    "    logger.error(ec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import common.widgets_utility as widgets_utility\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(LdaDataCompiler.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display topic's word distribution\n",
    "if False:\n",
    "    from common.model_utility import ModelUtility\n",
    "    from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "    from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "    import math\n",
    "\n",
    "    from itertools import product\n",
    "    \n",
    "    import bokeh.models as bm\n",
    "    import bokeh.palettes\n",
    "    from bokeh.io import output_file, push_notebook\n",
    "    from bokeh.core.properties import value, expr\n",
    "    from bokeh.transform import transform, jitter\n",
    "    from bokeh.layouts import row, column, widgetbox\n",
    "    from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "    from bokeh.models import ColumnDataSource, CustomJS\n",
    "    \n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "    widget_container.reset()\n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "TM_DATA = TM_GUI_MODEL.model\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, TM_DATA, 'wc01', ['Chart', 'Table'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "#import numpy as np\n",
    "#import math\n",
    "#import bokeh.plotting\n",
    "#from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "#from bokeh.models.glyphs import VBar\n",
    "#from bokeh.io import curdoc, show\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=tools, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, widgets_container, output_format='Chart', tm_data=None, threshold=0.01):\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=200)\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = 'weight'\n",
    "    category_column = 'author'\n",
    "    df = container.document_topic_weights[(container.document_topic_weights.topic_id==topic_id)]\n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    df[category_column] = df.title.apply(slim_title)\n",
    "\n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = category_column.title()\n",
    "        y_label = value_column.title()\n",
    "        p = plot_topic_trend(df, category_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=TOOLS)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_network(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    text_widget = widgets_utility.wf.create_text_widget(text_id)  # style=\"display: inline; height='400px'\"),\n",
    "    scale_widget = widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False)\n",
    "    threshold_widget = widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False)\n",
    "    output_format_widget = widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network')\n",
    "    layout_widget = widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold')\n",
    "    progress_widget = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    \n",
    "    def tick(x=None):\n",
    "        progress_widget.value = progress_widget.value + 1 if x is None else x\n",
    "        \n",
    "    def display_topic_network(layout_algorithm, tm_data, threshold=0.10, scale=1.0, output_format='network'):\n",
    "            \n",
    "        tick(1)\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "        df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "        \n",
    "        df['slim_title'] = df.title.apply(slim_title)\n",
    "        network = NetworkUtility.create_bipartite_network(df, 'slim_title', 'topic_id')\n",
    "        \n",
    "        tick()\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            tick()\n",
    "            \n",
    "            p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "            bokeh.plotting.show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        tick(0)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_network,\n",
    "        layout_algorithm=layout_widget,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=threshold_widget,\n",
    "        scale=scale_widget,\n",
    "        output_format=output_format_widget\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        text_widget,\n",
    "        widgets.HBox([layout_widget, threshold_widget]), \n",
    "        widgets.HBox([output_format_widget, scale_widget, progress_widget]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "main_topic_network(tm_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Trends - Heatmap\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanfordâ€™s Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(title=\"Topic heatmap\", tools=TOOLS, toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "def display_doc_topic_heatmap(tm_data, key='max', flip_axis=False, glyph='Circle'):\n",
    "    try:\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights, n_words=100)\n",
    "        df = container.document_topic_weights.copy().reset_index()\n",
    "        df['document_id'] = df.document_id.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "        df['author'] = df.title.apply(slim_title)\n",
    "        p = plot_topic_relevance_by_year(df, xs='author', ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "        bokeh.plotting.show(p)\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "            \n",
    "def doc_topic_heatmap_gui(tm_data):\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "\n",
    "    text_id = 'topic_relevance'\n",
    "    #text_widget = widgets_utility.wf.create_text_widget(text_id)\n",
    "    text_widget = text_widget(text_id)\n",
    "    glyph = widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\"))\n",
    "    flip_axis = widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "\n",
    "    iw = widgets.interactive(display_doc_topic_heatmap, tm_data=widgets.fixed(tm_data), glyph=glyph, flip_axis=flip_axis)\n",
    "\n",
    "    display(widgets.VBox([widgets.HBox([flip_axis, glyph ]), text_widget, iw.children[-1]]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "doc_topic_heatmap_gui(get_current_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Key Terms \n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.keyterms\n",
    "\n",
    "def display_document_key_terms_gui(corpus):\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "    methods = { 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Paper', options=document_options, value=0, layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=0, normalize='lemma', n_keyterms=10):\n",
    "        keyterms = methods[method](corpus[document_id], normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ' '.join([ x for x, y in keyterms ])\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            display(terms)\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CORPUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}