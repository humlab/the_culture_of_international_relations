{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Culture of International Relations - Corpus statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import bokeh, bokeh.plotting, bokeh.models, matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "import types, glob\n",
    "import textacy.keyterms\n",
    "import qgrid\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "#pd.options.display.max_colwidth = -1\n",
    "pd.options.display.colheader_justify = 'left'\n",
    "#pd.options.display.precision = 4\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "LANGUAGE_MAP = { 'en': 'english', 'fr': 'french', 'it': 'other', 'de': 'other' }\n",
    "LANGUAGE_MODEL_MAP = { 'en': 'en_core_web_sm', 'fr': 'fr_core_web_sm', 'it': 'it_core_web_sm', 'de': 'de_core_web_sm' }\n",
    "\n",
    "FIXED_STOPWORDS = ['', '\\n', 'et', 'al', 'et.al.' ]\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "\n",
    "# sudo python3 -m spacy download en_core_web_lg\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_filenames(zip_filename, extension='.txt'):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return [ x for x in zf.namelist() if x.endswith(extension) ]\n",
    "    \n",
    "def get_text(zip_filename, filename):\n",
    "    with zipfile.ZipFile(zip_filename, mode='r') as zf:\n",
    "        return zf.read(filename).decode(encoding='utf-8')\n",
    "    \n",
    "# TODO: Move to Treaty data utilities\n",
    "def get_treaties(wti_index, language, period_group='years_1945-1972', treaty_filter='is_cultural', parties=None):\n",
    "    period_group = config.PERIOD_GROUPS_ID_MAP[period_group]\n",
    "    treaties = wti_index.get_treaties_within_division(\n",
    "        period_group=period_group,\n",
    "        treaty_filter=treaty_filter,\n",
    "        recode_is_cultural=False,\n",
    "        parties=parties\n",
    "    )\n",
    "    treaties = treaties[treaties[LANGUAGE_MAP[language]]==language]\n",
    "    return treaties\n",
    "\n",
    "DEFAULT_TERM_PARAMS = dict(\n",
    "    args=dict(ngrams=1, named_entities=True, normalize='lemma', as_strings=True),\n",
    "    kwargs=dict(filter_stops=True, filter_punct=True, filter_nums=True, min_freq=1, drop_determiners=True, include_pos=('NOUN', 'PROPN', ))\n",
    ")\n",
    "\n",
    "class TopicModelNotComputed(Exception):\n",
    "    @staticmethod\n",
    "    def check():\n",
    "        if 'TM_GUI_MODEL' in globals():\n",
    "            gui =  globals()['TM_GUI_MODEL']\n",
    "            if None not in (gui, gui.model):\n",
    "                return True\n",
    "        msg = 'A topic model must be computed using step \"MODEL Compute an LDA Topic Model\"'\n",
    "        raise TopicModelNotComputed(msg)\n",
    "\n",
    "def get_current_model():\n",
    "    TopicModelNotComputed.check()\n",
    "    return globals()['TM_GUI_MODEL'].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: %writefile: spacy_utility\n",
    "from spacy.language import Language\n",
    "from textacy.spacier.utils import merge_spans\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "\n",
    "def preprocess_text(source_filename, target_filename, tick=utility.noop):\n",
    "    filenames = get_filenames(source_filename)\n",
    "    texts = ( (filename, get_text(source_filename, filename)) for filename in filenames )\n",
    "    logger.info('Preparing text corpus...')\n",
    "    tick(0, len(filenames))\n",
    "    with zipfile.ZipFile(target_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for filename, text in texts:\n",
    "            text = re.sub(HYPHEN_REGEXP, r\"\\1\\2\\n\", text)\n",
    "            text = textacy.preprocess.normalize_whitespace(text)   \n",
    "            text = textacy.preprocess.fix_bad_unicode(text)   \n",
    "            text = textacy.preprocess.replace_currency_symbols(text)\n",
    "            text = textacy.preprocess.unpack_contractions(text)\n",
    "            text = textacy.preprocess.remove_accents(text)\n",
    "            zf.writestr(filename, text)\n",
    "            tick()\n",
    "    tick(0)\n",
    "\n",
    "def textacy_filter_terms(doc, term_args, chunk_size=None, min_length=2):\n",
    "    kwargs = utility.extend({}, DEFAULT_TERM_PARAMS['kwargs'], term_args['kwargs'])\n",
    "    args = utility.extend({}, DEFAULT_TERM_PARAMS['args'], term_args['args'])\n",
    "    terms = (x for x in doc.to_terms_list(\n",
    "        args['ngrams'],\n",
    "        args['named_entities'],\n",
    "        args['normalize'],\n",
    "        args['as_strings'],\n",
    "        **kwargs\n",
    "    ) if len(x) >= min_length and x not in FIXED_STOPWORDS)\n",
    "    return terms\n",
    "\n",
    "def generate_treaty_id_option_list(corpus):\n",
    "    \n",
    "    def format_treaty_name(x):\n",
    "        return '{}: {} {} {} {}'.format(x.name, x['signed_year'], x['topic'], x['party1'], x['party2'])\n",
    "    \n",
    "    documents = WTI_INDEX.treaties.loc[get_corpus_documents(corpus).title]\n",
    "    \n",
    "    options = [ (v, k) for k, v in documents.apply(format_treaty_name, axis=1).to_dict().items() ]\n",
    "    options = sorted(options, key=lambda x: x[0])\n",
    "    \n",
    "    return options\n",
    "\n",
    "def get_treaty_doc(corpus, treaty_id):\n",
    "    for doc in corpus.get(lambda x: x.metadata['treaty_id'] == treaty_id, limit=1):\n",
    "        return doc\n",
    "    return None\n",
    "\n",
    "def get_document_stream(corpus_path, lang, treaties):\n",
    "    \n",
    "    if 'treaty_id' not in treaties.columns:\n",
    "        treaties['treaty_id'] = treaties.index\n",
    "        \n",
    "    documents = treaty_corpus.TreatyCompressedFileReader(corpus_path, lang, list(treaties.index))\n",
    "    \n",
    "    for treaty_id, language, filename, text in documents:\n",
    "        assert language == lang\n",
    "        metadata = treaties.loc[treaty_id]\n",
    "        yield filename, text, metadata\n",
    "        \n",
    "def create_textacy_corpus(documents, nlp, tick=utility.noop):\n",
    "    corpus = textacy.Corpus(nlp)\n",
    "    for filename, text, metadata in documents:\n",
    "        corpus.add_text(text, utility.extend(dict(filename=filename), metadata))\n",
    "        tick()\n",
    "    return corpus\n",
    "\n",
    "def compute_textacy_corpus_filename(source_path, language, nlp_args=None, preprocess_args=None):\n",
    "    nlp_args = nlp_args or {}\n",
    "    preprocess_args = preprocess_args or {}\n",
    "    disabled_pipes = nlp_args.get('disable', [])\n",
    "    suffix = '_{}_{}{}'.format(\n",
    "        language,\n",
    "        '_'.join([ k for k in preprocess_args if preprocess_args[k] ]),\n",
    "        '_disable({})'.format(','.join(disabled_pipes)) if len(disabled_pipes) > 0 else ''\n",
    "    )\n",
    "    return utility.path_add_suffix(source_path, suffix, new_extension='.pkl')\n",
    "\n",
    "def setup_nlp_language_model(language, **nlp_args):\n",
    "    \n",
    "    def remove_whitespace_entities(doc):\n",
    "        doc.ents = [ e for e in doc.ents if not e.text.isspace() ]\n",
    "        return doc\n",
    "\n",
    "    logger.info('Loading model: %s...', language)\n",
    "    \n",
    "    Language.factories['remove_whitespace_entities'] = lambda nlp, **cfg: remove_whitespace_entities\n",
    "    \n",
    "    nlp = textacy.load_spacy(LANGUAGE_MODEL_MAP[language], **nlp_args)\n",
    "    pipeline = lambda: [ x[0] for x in nlp.pipeline ]\n",
    "    \n",
    "    logger.info('Using pipeline: ' + ' '.join(pipeline()))\n",
    "    \n",
    "    return nlp\n",
    "\n",
    "def propagate_document_attributes(corpus):\n",
    "    for doc in corpus:\n",
    "        doc.spacy_doc.user_data['title'] = doc.metadata['treaty_id']\n",
    "        doc.spacy_doc.user_data['treaty_id'] = doc.metadata['treaty_id']\n",
    "    \n",
    "def get_corpus_documents(corpus):\n",
    "    df = pd.DataFrame([\n",
    "        (document_id, doc.metadata['treaty_id'], doc.metadata['filename'])\n",
    "                for document_id, doc in enumerate(corpus) ], columns=['treaty_id', 'title', 'filename']\n",
    "    ).set_index('treaty_id')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_textacy_corpus(\n",
    "    data_folder,\n",
    "    wti_index,\n",
    "    container,\n",
    "    gui,\n",
    "    source_path,\n",
    "    language,\n",
    "    merge_entities,\n",
    "    overwrite=False,\n",
    "    period_group='years_1945-1972',\n",
    "    treaty_filter='',\n",
    "    parties=None,\n",
    "    disabled_pipes=None\n",
    "):\n",
    "    \n",
    "    def tick(step=None, max_step=None):\n",
    "        \n",
    "        if max_step is not None:\n",
    "            gui.progress.max = max_step\n",
    "            \n",
    "        gui.progress.value = gui.progress.value + 1 if step is None else step\n",
    "        \n",
    "    for key in container.__dict__:\n",
    "        container.__dict__[key] = None\n",
    "        \n",
    "    nlp_args = { 'disable': disabled_pipes or [] }\n",
    "    \n",
    "    container.source_path = source_path\n",
    "    container.language = language\n",
    "    container.textacy_corpus = None\n",
    "    container.prepped_source_path = utility.path_add_suffix(source_path, '_preprocessed')\n",
    "    \n",
    "    if not os.path.isfile(container.prepped_source_path):\n",
    "        preprocess_text(container.source_path, container.prepped_source_path, tick=tick)\n",
    "        \n",
    "    container.textacy_corpus_path = compute_textacy_corpus_filename(container.prepped_source_path, container.language, nlp_args=nlp_args)\n",
    "    \n",
    "    container.nlp = setup_nlp_language_model(container.language, **nlp_args)\n",
    "    \n",
    "    if overwrite or not os.path.isfile(container.textacy_corpus_path):\n",
    "        \n",
    "        logger.info('Working: Computing new corpus ' + container.textacy_corpus_path + '...')\n",
    "        \n",
    "        treaties = get_treaties(wti_index, language=container.language, period_group=period_group, treaty_filter=treaty_filter, parties=parties)\n",
    "        stream = get_document_stream(container.prepped_source_path, container.language, treaties)\n",
    "        \n",
    "        logger.info('Working: Stream created...')\n",
    "        \n",
    "        tick(0, len(treaties))\n",
    "        container.textacy_corpus = create_textacy_corpus(stream, container.nlp, tick)\n",
    "        container.textacy_corpus.save(container.textacy_corpus_path)\n",
    "        tick(0)\n",
    "        \n",
    "    else:\n",
    "        logger.info('Working: Loading corpus ' + container.textacy_corpus_path + '...')\n",
    "        tick(1,2)\n",
    "        container.textacy_corpus = textacy.Corpus.load(container.textacy_corpus_path)\n",
    "        tick(0)\n",
    "        \n",
    "    if merge_entities:\n",
    "        logger.info('Working: Merging named entities...')\n",
    "        for doc in container.textacy_corpus:\n",
    "            named_entities = textacy.extract.named_entities(doc)\n",
    "            merge_spans(named_entities, doc.spacy_doc)\n",
    "    else:\n",
    "        logger.info('Named entities not merged')\n",
    "        \n",
    "    logger.info('Done!')\n",
    "    \n",
    "def display_corpus_load_gui(data_folder, wti_index, container, compute_callback):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    language_options = { LANGUAGE_MAP[k].title(): k for k in LANGUAGE_MAP.keys() }\n",
    "    period_group_options = { config.PERIOD_GROUPS_ID_MAP[k]['title']: k for k in config.PERIOD_GROUPS_ID_MAP }\n",
    "    \n",
    "    corpus_files = sorted(glob.glob(os.path.join(data_folder, 'treaty_text_corpora_????????.zip')))\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        \n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('90%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        source_path=widgets_config.dropdown(description='Corpus', options=corpus_files, value=corpus_files[-1], layout=lw('300px')),\n",
    "        \n",
    "        language=widgets_config.dropdown(description='Language', options=language_options, value='en', layout=lw('180px')),\n",
    "        period_group=widgets_config.dropdown('Period', period_group_options, 'years_1945-1972', layout=lw('180px')),\n",
    "\n",
    "        merge_entities=widgets_config.toggle('Merge Entities', True, icon='', layout=lw('100px')),\n",
    "        overwrite=widgets_config.toggle('Force', False, icon='', layout=lw('100px'), tooltip=\"Force generation of new corpus (even if exists)\"),\n",
    "        \n",
    "        compute_pos=widgets_config.toggle('POS', True, icon='', layout=lw('100px'), disabled=True, tooltip=\"Enable Part-of-Speech tagging\"),\n",
    "        compute_ner=widgets_config.toggle('NER', False, icon='', layout=lw('100px'), disabled=False, tooltip=\"Enable NER tagging\"),\n",
    "        compute_dep=widgets_config.toggle('DEP', False, icon='', layout=lw('100px'), disabled=True, tooltip=\"Enable dependency parsing\"),\n",
    "        \n",
    "        compute=widgets.Button(description='Compute', layout=lw('100px'))\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            gui.source_path,\n",
    "            widgets.VBox([\n",
    "                gui.language,\n",
    "                gui.period_group\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.merge_entities,\n",
    "                gui.overwrite\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.compute_pos,\n",
    "                gui.compute_ner,\n",
    "                gui.compute_dep\n",
    "            ]),\n",
    "            gui.compute]),\n",
    "        gui.output\n",
    "    ]))\n",
    "    \n",
    "    def compute_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            disabled_pipes = (() if gui.compute_pos.value else (\"tagger\",)) + \\\n",
    "                             (() if gui.compute_dep.value else (\"parser\",)) + \\\n",
    "                             (() if gui.compute_ner.value else (\"ner\",))\n",
    "            generate_textacy_corpus(\n",
    "                data_folder=data_folder,\n",
    "                wti_index=wti_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                source_path=gui.source_path.value,\n",
    "                language=gui.language.value,\n",
    "                merge_entities=gui.merge_entities.value,\n",
    "                overwrite=gui.overwrite.value,\n",
    "                period_group=gui.period_group.value,\n",
    "                parties=None,\n",
    "                disabled_pipes=tuple(disabled_pipes)\n",
    "            )\n",
    "\n",
    "    gui.compute.on_click(compute_callback)\n",
    "\n",
    "CURRENT_CORPUS = types.SimpleNamespace(\n",
    "    language=None,\n",
    "    source_path=None,\n",
    "    prepped_source_path=None,\n",
    "    textacy_corpus_path=None,\n",
    "    textacy_corpus=None,\n",
    "    nlp=None\n",
    ")\n",
    "\n",
    "display_corpus_load_gui(DATA_FOLDER, WTI_INDEX, CURRENT_CORPUS, generate_textacy_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Find Key Terms <span style='float: right; color: green'>OPTIONAL</span>\n",
    "- [TextRank]\tMihalcea, R., & Tarau, P. (2004, July). TextRank: Bringing order into texts. Association for Computational Linguistics.\n",
    "- [SingleRank]\tHasan, K. S., & Ng, V. (2010, August). Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (pp. 365-373). Association for Computational Linguistics.\n",
    "- [RAKE]\tRose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Son\n",
    "https://github.com/csurfer/rake-nltk\n",
    "https://github.com/aneesha/RAKE\n",
    "https://github.com/vgrabovets/multi_rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span>RAKE <span style='float: right; color: green'>OPTIONAL</span>\n",
    "\n",
    "\n",
    "https://github.com/JRC1995/TextRank-Keyword-Extraction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Document Key Terms\n",
    "from rake_nltk import Rake, Metric\n",
    "\n",
    "#https://github.com/JRC1995/RAKE-Keyword-Extraction\n",
    "    \n",
    "def textacy_rake(doc, language='english', normalize='lemma', n_keyterms=20, stopwords=None):\n",
    "\n",
    "    r = Rake(\n",
    "        stopwords=stopwords,  # NLTK stopwords if None\n",
    "        punctuations=None, # NLTK by default\n",
    "        language=\"english\",\n",
    "        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n",
    "        max_length=100000,\n",
    "        min_length=1\n",
    "    )\n",
    "    r.extract_keywords_from_text(doc.text)\n",
    "    \n",
    "    # keyterms = r.get_ranked_phrases()\n",
    "    keyterms = [ (y, x) for (x, y) in r.get_ranked_phrases_with_scores() ]\n",
    "\n",
    "    return keyterms\n",
    "\n",
    "doc = CURRENT_CORPUS.textacy_corpus[0]\n",
    "textacy_rake(doc)\n",
    "#doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def display_document_key_terms_gui(corpus):\n",
    "    \n",
    "    methods = { 'RAKE': textacy_rake, 'SingleRank': textacy.keyterms.singlerank, 'TextRank': textacy.keyterms.textrank }\n",
    "    # df_documents = get_corpus_documents(corpus)\n",
    "    # document_options = {v: k for k, v in df_documents['title'].to_dict().items()}\n",
    "    document_options = [('All Treaties', None)] + generate_treaty_id_option_list(corpus)\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(min=0, max=1, step=1, layout=widgets.Layout(width='95%')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'}),\n",
    "        n_keyterms=widgets.IntSlider(description='#words', min=10, max=500, value=100, step=1, layout=widgets.Layout(width='240px')),\n",
    "        document_id=widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='40%')),\n",
    "        method=widgets.Dropdown(description='Algorithm', options=[ 'RAKE', 'TextRank', 'SingleRank' ], value='TextRank', layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ 'lemma', 'lower' ], value='lemma', layout=widgets.Layout(width='160px'))\n",
    "    )\n",
    "    \n",
    "    def get_keyterms(method, doc, normalize, n_keyterms):\n",
    "        keyterms = methods[method](doc, normalize=normalize, n_keyterms=n_keyterms)\n",
    "        terms = ', '.join([ x for x, y in keyterms ])\n",
    "        gui.progress.value += 1\n",
    "        return terms\n",
    "    \n",
    "    def get_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        treaty_ids = [ document_id ] if document_id is not None else [ doc.metadata['treaty_id'] for doc in corpus ]\n",
    "        gui.progress.value = 0\n",
    "        gui.progress.max = len(treaty_ids)\n",
    "        keyterms = [\n",
    "            get_keyterms(method, get_treaty_doc(corpus, treaty_id), normalize, n_keyterms) for treaty_id in treaty_ids\n",
    "        ]\n",
    "        df = pd.DataFrame({ 'treaty_id': treaty_ids, 'keyterms': keyterms}).set_index('treaty_id')\n",
    "        gui.progress.value = 0\n",
    "        return df\n",
    "\n",
    "    def display_document_key_terms(corpus, method='TextRank', document_id=None, normalize='lemma', n_keyterms=10):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df = get_document_key_terms(corpus, method, document_id, normalize, n_keyterms)\n",
    "            #qgrid_widget = qgrid.show_grid(df, show_toolbar=False)\n",
    "            #display(qgrid_widget)\n",
    "            table = TableDisplay(df)\n",
    "            display(table)\n",
    "            \n",
    "    itw = widgets.interactive(\n",
    "        display_document_key_terms,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        method=gui.method,\n",
    "        document_id=gui.document_id,\n",
    "        normalize=gui.normalize,\n",
    "        n_keyterms=gui.n_keyterms,\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([gui.document_id, gui.method, gui.normalize, gui.n_keyterms]),\n",
    "        gui.output\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "\n",
    "display_document_key_terms_gui(CURRENT_CORPUS.textacy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE/DESCRIBE </span> Clean Up the Text <span style='float: right; color: green'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Clean Up the Text\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':'tight'}\n",
    "\n",
    "HYPHEN_REGEXP = re.compile(r'\\b(\\w+)-\\s*\\r?\\n\\s*(\\w+)\\b', re.UNICODE)\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "\n",
    "def display_cleanup_text_gui(container, callback):\n",
    "    corpus = container.textacy_corpus\n",
    "    document_options = generate_treaty_id_option_list(corpus)\n",
    "    \n",
    "    #pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]  # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    pos_tags = DF_TAGSET.groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    display_options = {\n",
    "        'Source text (raw)': 'source_text_raw',\n",
    "        'Source text (edited)': 'source_text_edited',\n",
    "        'Source text (processed)': 'source_text_preprocessed',\n",
    "        'Sanitized text': 'sanitized_text',\n",
    "        'Statistics': 'statistics'\n",
    "    }\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    gui = types.SimpleNamespace(\n",
    "        treaty_id=widgets.Dropdown(description='Treaty', options=document_options, value=None, layout=widgets.Layout(width='400px')),\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        min_freq=widgets.FloatSlider(value=0, min=0, max=1.0, step=0.01, description='Min frequency', layout=widgets.Layout(width='400px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=[ False, 'lemma', 'lower' ], value=False, layout=widgets.Layout(width='180px')),\n",
    "        filter_stops=widgets.ToggleButton(value=False, description='Filter stops',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=False, description='Filter nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_punct=widgets.ToggleButton(value=False, description='Filter punct',  tooltip='Filter out punctuations', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=False, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(), rows=10, layout=widgets.Layout(width='400px')),\n",
    "        display_type=widgets.Dropdown(description='Show', value='statistics', options=display_options, layout=widgets.Layout(width='180px')),\n",
    "        output_text=widgets.Output(layout={'height': '500px'}),\n",
    "        output_statistics = widgets.Output(),\n",
    "        boxes=None\n",
    "    )\n",
    "    \n",
    "    uix = widgets.interactive(\n",
    "\n",
    "        callback,\n",
    "\n",
    "        container=widgets.fixed(container),\n",
    "        gui=widgets.fixed(gui),\n",
    "        display_type=gui.display_type,\n",
    "        treaty_id=gui.treaty_id,\n",
    "        \n",
    "        ngrams=gui.ngrams,\n",
    "        named_entities=gui.named_entities,\n",
    "        normalize=gui.normalize,\n",
    "        filter_stops=gui.filter_stops,\n",
    "        filter_punct=gui.filter_punct,\n",
    "        filter_nums=gui.filter_nums,\n",
    "        include_pos=gui.include_pos,\n",
    "        min_freq=gui.min_freq,\n",
    "        drop_determiners=gui.drop_determiners\n",
    "    )\n",
    "    \n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.treaty_id,\n",
    "                widgets.HBox([gui.display_type, gui.normalize]),\n",
    "                widgets.HBox([gui.ngrams, gui.min_word]),\n",
    "                gui.min_freq\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.filter_punct,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.HBox([\n",
    "            gui.output_text, gui.output_statistics\n",
    "        ]),\n",
    "        uix.children[-1]\n",
    "    ])\n",
    "    \n",
    "    display(gui.boxes)\n",
    "                                  \n",
    "    uix.update()\n",
    "    return gui, uix\n",
    "\n",
    "def plot_xy_data(data, title='', xlabel='', ylabel='', **kwargs):\n",
    "    x, y = list(data[0]), list(data[1])\n",
    "    labels = x\n",
    "    plt.figure(figsize=(8, 9 / 1.618))\n",
    "    plt.plot(x, y, 'ro', **kwargs)\n",
    "    plt.xticks(x, labels, rotation='75')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def display_cleaned_up_text(container, gui, display_type, treaty_id, **kwargs): # ngrams, named_entities, normalize, include_pos):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    gui.output_text.clear_output()\n",
    "    gui.output_statistics.clear_output()\n",
    "    \n",
    "    doc = get_treaty_doc(corpus, treaty_id)\n",
    "    if doc is None:\n",
    "        return\n",
    "    \n",
    "    terms = [ x for x in doc.to_terms_list(as_strings=True, **kwargs) ]\n",
    "    \n",
    "    if display_type.startswith('source_text'):\n",
    "        \n",
    "        source_files = {\n",
    "            'source_text_raw': { 'filename': container.source_path, 'description': 'Raw text from PDF: Automatic text extraction using pdfminer Python package. ' },\n",
    "            'source_text_edited': { 'filename': container.source_path, 'description': 'Manually edited text: List of references, index, notes and page headers etc. removed.' },\n",
    "            'source_text_preprocessed': { 'filename': container.prepped_source_path, 'description': 'Preprocessed text: Normalized whitespaces. Unicode fixes. Urls, emails and phonenumbers removed. Accents removed.' }\n",
    "        }        \n",
    "        \n",
    "        source_filename = source_files[display_type]['filename']\n",
    "        description =  source_files[display_type]['description']\n",
    "        text = get_text(source_filename, doc.metadata['filename'])\n",
    "        \n",
    "        with gui.output_text:\n",
    "            #print('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(document[:2500], document[-250:]))\n",
    "            #print(doc)\n",
    "            print('[ ' + description.upper() + ' ]')\n",
    "            print(text)\n",
    "        return\n",
    "\n",
    "    if len(terms) == 0:\n",
    "        with gui.output_text:\n",
    "            print(\"No text. Please change selection.\")\n",
    "        return\n",
    "    \n",
    "    if display_type in ['sanitized_text', 'statistics']:\n",
    "\n",
    "        if display_type == 'sanitized_text':\n",
    "            with gui.output_text:\n",
    "                #display('{}\\n.................\\n(NOT SHOWN TEXT)\\n.................\\n{}'.format(\n",
    "                #    ' '.join(tokens[:word_count]),\n",
    "                #    ' '.join(tokens[-word_count:])\n",
    "                #))\n",
    "                print(' '.join([ t.replace(' ', '_') for t in terms ]))\n",
    "                return\n",
    "\n",
    "        if display_type == 'statistics':\n",
    "\n",
    "            wf = nltk.FreqDist(terms)\n",
    "\n",
    "            with gui.output_text:\n",
    "\n",
    "                df = pd.DataFrame(wf.most_common(25), columns=['token','count'])\n",
    "                print('Token count: {} Vocab count: {}'.format(wf.N(), wf.B()))\n",
    "                display(df)\n",
    " \n",
    "            with gui.output_statistics:\n",
    "\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word distribution', xlabel='Word', ylabel='Word count')\n",
    "\n",
    "                wf = nltk.FreqDist([len(x) for x in terms])\n",
    "                data = list(zip(*wf.most_common(25)))\n",
    "                plot_xy_data(data, title='Word length distribution', xlabel='Word length', ylabel='Word count')\n",
    "\n",
    "xgui, xuix = display_cleanup_text_gui(CURRENT_CORPUS, display_cleaned_up_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> List of most frequent words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import attrs\n",
    "\n",
    "ADDITIONAL_STOPWORDS = []\n",
    "\n",
    "def textacy_doc_to_bow(doc, target='lemma', weighting='count', as_strings=False, include=None):\n",
    "\n",
    "    spacy_doc = doc.spacy_doc\n",
    "    \n",
    "    weighing_keys = { 'count', 'freq' }\n",
    "    target_keys = { 'lemma': attrs.LEMMA, 'lower': attrs.LOWER, 'orth': attrs.ORTH }\n",
    "    \n",
    "    default_exclude = lambda x: x.is_stop or x.is_punct or x.is_space\n",
    "    exclude = default_exclude if include is None else lambda x: default_exclude(x) or not include(x)\n",
    "    \n",
    "    assert weighting in weighing_keys\n",
    "    assert target in target_keys\n",
    "\n",
    "    target_weights = spacy_doc.count_by(target_keys[target], exclude=exclude)\n",
    "    \n",
    "    if weighting == 'freq':\n",
    "        n_tokens = sum(target_weights.values())\n",
    "        target_weights = {id_: weight / n_tokens for id_, weight in target_weights.items()}\n",
    "\n",
    "    if as_strings:\n",
    "        bow = { doc.spacy_stringstore[word_id]: count for word_id, count in target_weights.items() }\n",
    "    else:\n",
    "        bow = { word_id: count for word_id, count in target_weights.items() }\n",
    "        \n",
    "    return bow\n",
    "    \n",
    "#def word_weights(corpus, docs, normalize='lemma'):\n",
    "#    \n",
    "#    docs = list(docs)\n",
    "#    word_weights = collections.Counter()\n",
    "#    \n",
    "#    for doc in docs:\n",
    "#        textacy_filter_terms(doc, term_args, chunk_size=None, min_length=2)\n",
    "#        word_weights.update(doc.to_bag_of_words(normalize=normalize, weighting='freq', as_strings=False))\n",
    "#\n",
    "#    n_documents = len(docs)\n",
    "#    word_weights = { word: weight / n_documents for word, weight in word_weights.items() }\n",
    "#    \n",
    "#    df = pd.DataFrame({'word_id': list(word_weights.keys()),  'weight': list(word_weights.values()) })\n",
    "#    df['token'] = df.word_id.apply(lambda x: corpus.spacy_vocab[x].text)\n",
    "#    \n",
    "#    return df\n",
    "\n",
    "def trunc_year_by(series, divisor):\n",
    "    return (series - series.mod(divisor)).astype(int) \n",
    "\n",
    "TREATY_TIME_GROUPINGS = {\n",
    "    'treaty_id': { 'column': 'treaty_id', 'divisor': None, 'title': 'Treaty', 'fx': None},\n",
    "    'signed_year': { 'column': 'signed_year', 'divisor': 1, 'title': 'Year', 'fx': None },\n",
    "    'signed_lustrum': { 'column': 'signed_lustrum', 'divisor': 5, 'title': 'Lustrum', 'fx': lambda df: trunc_year_by(df.signed_year, 5) },\n",
    "    'signed_decade': { 'column': 'signed_decade', 'divisor': 10, 'title': 'Decade', 'fx': lambda df: trunc_year_by(df.signed_year, 10) }\n",
    "}\n",
    "\n",
    "def display_list_of_most_frequent_words(\n",
    "    data_folder,\n",
    "    wti_index,\n",
    "    container,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    target='lemma',\n",
    "    weighting='count',\n",
    "    include_pos=None,\n",
    "    stop_words=None\n",
    "):\n",
    "    \n",
    "    corpus = container.textacy_corpus\n",
    "    stop_words = stop_words or set()\n",
    "    \n",
    "    def include(token):\n",
    "        flag = True\n",
    "        if not include_pos is None:\n",
    "             flag = flag and token.pos_ in include_pos\n",
    "        flag = flag and token.lemma_ not in stop_words\n",
    "        return flag\n",
    "    \n",
    "    gui.progress.max = len(corpus)\n",
    "    \n",
    "    df_freqs = pd.DataFrame({ 'treaty_id': [], 'signed_year': [], 'token': [], 'count': [] })\n",
    "    \n",
    "    for doc in corpus:\n",
    "        \n",
    "        doc_freqs = textacy_doc_to_bow(doc, target=target, weighting=weighting, as_strings=True, include=include)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'treaty_id': doc.metadata['treaty_id'],\n",
    "            'signed_year': int(doc.metadata['signed_year']),\n",
    "            'token': list(doc_freqs.keys()),\n",
    "            'count': list(doc_freqs.values())\n",
    "        })#.nlargest(n_tokens, 'count')\\\n",
    "          #.reset_index().reset_index()\\\n",
    "          #.rename(columns={'level_0': 'position'})\\\n",
    "          #.drop('index', axis=1)\n",
    "        \n",
    "        df_freqs = df_freqs.append(df)\n",
    "        gui.progress.value = gui.progress.value + 1\n",
    "        \n",
    "    df_freqs['signed_year'] = df_freqs.signed_year.astype(int)\n",
    "    \n",
    "    for key, group in TREATY_TIME_GROUPINGS.items():\n",
    "        if key in df_freqs.columns:\n",
    "            continue\n",
    "        df_freqs[key] = (group['fx'])(df_freqs)\n",
    "        \n",
    "    df_freqs['term'] = df_freqs.token # if True else df_freqs.token\n",
    "    \n",
    "    df_freqs = df_freqs.groupby([group_by_column, 'term']).mean().reset_index()[[group_by_column, 'term', 'count']]\n",
    "    \n",
    "    df_freqs['position'] = df_freqs.sort_values(by=[group_by_column, 'count'], ascending=False).groupby([group_by_column]).cumcount() + 1\n",
    "    \n",
    "    gui.progress.value = 0\n",
    "    \n",
    "    return df_freqs\n",
    "    \n",
    "def get_most_frequent_words(corpus, n_top, normalize='lemma', include_pos=None):\n",
    "    include_pos = include_pos or [ 'VERB', 'NOUN', 'PROPN' ]\n",
    "    include = lambda x: x.pos_ in include_pos\n",
    "    word_counts = collections.Counter()\n",
    "    for doc in corpus:\n",
    "        bow = textacy_doc_to_bow(doc, target=normalize, weighting='count', as_strings=True, include=include)\n",
    "        word_counts.update(bow)\n",
    "    return word_counts.most_common(n_top)\n",
    "\n",
    "def list_of_most_frequent_words_gui(data_folder, wti_index, container):\n",
    "\n",
    "    corpus = container.textacy_corpus\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    \n",
    "    #pos_tags = DF_TAGSET[DF_TAGSET.POS.isin(include_pos_tags)].groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    #pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    counter = collections.Counter(corpus.word_freqs(normalize='lemma', weighting='count', as_strings=True))\n",
    "    frequent_words = [ x[0] for x in get_most_frequent_words(corpus, 100) ]\n",
    "\n",
    "    group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    output_type_options = [\n",
    "        ( 'List', 'table' ),\n",
    "        ( 'Pivot', 'pivot' ),\n",
    "        ( 'Excel', 'excel' ),\n",
    "    ]\n",
    "    ngrams_options = { '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        parties=widgets_config.parties_widget(options=[ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ], value=None),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw(width='200px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='180px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=widgets.Layout(width='180px')),\n",
    "        target=widgets.Dropdown(description='Normalize', options={ '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }, value='lemma', layout=widgets.Layout(width='180px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options={ 'Count': 'count', 'Frequency': 'freq' }, value='freq', layout=widgets.Layout(width='180px')),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check', layout=widgets.Layout(width='140px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=list(['NOUN']), rows=7, layout=widgets.Layout(width='150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=widgets.Layout(width='200px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=widgets.Layout(width='180px')),\n",
    "        output_type=widgets.Dropdown(description='Output', value='pivot', options=output_type_options, layout=widgets.Layout(width='180px')),\n",
    "        n_tokens=widgets.IntSlider(description='#tokens', value=25, min=3, max=500, layout=widgets.Layout(width='250px')),\n",
    "        compute=widgets.Button(description='Compute', layout=widgets.Layout(width='140px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([gui.target]),\n",
    "                widgets.HBox([gui.ngrams]),\n",
    "                widgets.HBox([gui.weighting]),\n",
    "                widgets.HBox([gui.group_by_column]),\n",
    "                widgets.HBox([gui.party_preset]),\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.include_pos\n",
    "            ]),\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                #gui.named_entities,\n",
    "                gui.output_type,\n",
    "                gui.n_tokens,\n",
    "                gui.compute\n",
    "            ]),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def compute_callback(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            df_freqs = display_list_of_most_frequent_words(\n",
    "                data_folder=data_folder,\n",
    "                wti_index=wti_index,\n",
    "                container=container,\n",
    "                gui=gui,\n",
    "                target=gui.target.value,\n",
    "                group_by_column=gui.group_by_column.value,\n",
    "                weighting=gui.weighting.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                stop_words=set(gui.stop_words.value)\n",
    "            )\n",
    "            if gui.output_type.value == 'table':\n",
    "                display(df_freqs)\n",
    "            elif gui.output_type.value == 'pivot':\n",
    "                group_by_column = gui.group_by_column.value\n",
    "                df_freqs = df_freqs[df_freqs.position <= gui.n_tokens.value]\n",
    "                df_unstacked_freqs = df_freqs[[group_by_column, 'position', 'term']].set_index([group_by_column, 'position']).unstack()\n",
    "                display(df_unstacked_freqs)\n",
    "            else:\n",
    "                filename = '../data/word_trend_data.xlsx'\n",
    "                df_freqs.to_excel(filename)\n",
    "                print('Excel written: ' + filename)\n",
    "\n",
    "\n",
    "    gui.compute.on_click(compute_callback)\n",
    "    \n",
    "\n",
    "list_of_most_frequent_words_gui(DATA_FOLDER, WTI_INDEX, CURRENT_CORPUS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Compute an Topic Model<span style='color: red; float: right'>MANDATORY RUN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import types\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from gensim import corpora, models, matutils\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg') #'pdf', 'svg')\n",
    "    \n",
    "bokeh.plotting.output_notebook()\n",
    "\n",
    "class LdaDataCompiler():\n",
    "    \n",
    "    @staticmethod\n",
    "    def compile_dictionary(model):\n",
    "        logger.info('Compiling dictionary...')\n",
    "        token_ids, tokens = list(zip(*model.id2word.items()))\n",
    "        dfs = model.id2word.dfs.values() if model.id2word.dfs is not None else [0] * len(tokens)\n",
    "        dictionary = pd.DataFrame({\n",
    "            'token_id': token_ids,\n",
    "            'token': tokens,\n",
    "            'dfs': list(dfs)\n",
    "        }).set_index('token_id')[['token', 'dfs']]\n",
    "        return dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_weights(tm, dictionary, num_words=200):\n",
    "        logger.info('Compiling topic-tokens weights...')\n",
    "\n",
    "        df_topic_weights = pd.DataFrame(\n",
    "            [ (topic_id, token, weight)\n",
    "                for topic_id, tokens in (tm.show_topics(tm.num_topics, num_words=num_words, formatted=False))\n",
    "                    for token, weight in tokens if weight > 0.0 ],\n",
    "            columns=['topic_id', 'token', 'weight']\n",
    "        )\n",
    "\n",
    "        df = pd.merge(\n",
    "            df_topic_weights.set_index('token'),\n",
    "            dictionary.reset_index().set_index('token'),\n",
    "            how='inner',\n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        return df.reset_index()[['topic_id', 'token_id', 'token', 'weight']]\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_topic_token_overview(topic_token_weights, alpha=None, n_words=200):\n",
    "        \"\"\"\n",
    "        Group by topic_id and concatenate n_words words within group sorted by weight descending.\n",
    "        There must be a better way of doing this...\n",
    "        \"\"\"\n",
    "        logger.info('Compiling topic-tokens overview...')\n",
    "\n",
    "        df = topic_token_weights.groupby('topic_id')\\\n",
    "            .apply(lambda x: sorted(list(zip(x[\"token\"], x[\"weight\"])), key=lambda z: z[1], reverse=True))\\\n",
    "            .apply(lambda x: ' '.join([z[0] for z in x][:n_words])).reset_index()\n",
    "        df['alpha'] = df.topic_id.apply(lambda topic_id: alpha[topic_id]) if alpha is not None else 0.0\n",
    "        df.columns = ['topic_id', 'tokens', 'alpha']\n",
    "\n",
    "        return df.set_index('topic_id')\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_document_topics(model, corpus, documents, minimum_probability=0.001):\n",
    "\n",
    "        def document_topics_iter(model, corpus, minimum_probability):\n",
    "\n",
    "            if isinstance(model, models.LsiModel):\n",
    "                data_iter = model[corpus]\n",
    "            else:\n",
    "                data_iter = model.get_document_topics(corpus, minimum_probability=minimum_probability)\\\n",
    "                    if hasattr(model, 'get_document_topics')\\\n",
    "                    else model.load_document_topics()\n",
    "\n",
    "            for document_id, topic_weights in enumerate(data_iter):\n",
    "                for (topic_id, weight) in ((topic_id, weight) for (topic_id, weight) in topic_weights if weight >= minimum_probability):\n",
    "                    yield (document_id, topic_id, weight)\n",
    "        '''\n",
    "        Get document topic weights for all documents in corpus\n",
    "        Note!  minimum_probability=None filters less probable topics, set to 0 to retrieve all topcs\n",
    "\n",
    "        If gensim model then use 'get_document_topics', else 'load_document_topics' for mallet model\n",
    "        '''\n",
    "        logger.info('Compiling document topics...')\n",
    "        logger.info('  Creating data iterator...')\n",
    "        data = document_topics_iter(model, corpus, minimum_probability)\n",
    "        logger.info('  Creating frame from iterator...')\n",
    "        df_doc_topics = pd.DataFrame(data, columns=[ 'document_id', 'topic_id', 'weight' ]).set_index('document_id')\n",
    "        logger.info('  Merging data...')\n",
    "        df = pd.merge(documents, df_doc_topics, how='inner', left_index=True, right_index=True)\n",
    "        logger.info('  DONE!')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_compiled_data(model, corpus, id2term, documents):\n",
    "\n",
    "        dictionary = LdaDataCompiler.compile_dictionary(model)\n",
    "        topic_token_weights = LdaDataCompiler.compile_topic_token_weights(model, dictionary, num_words=200)\n",
    "        alpha = model.alpha if 'alpha' in model.__dict__ else None\n",
    "        topic_token_overview = LdaDataCompiler.compile_topic_token_overview(topic_token_weights, alpha)\n",
    "        document_topic_weights = LdaDataCompiler.compile_document_topics(model, corpus, documents, minimum_probability=0.001)\n",
    "\n",
    "        return types.SimpleNamespace(\n",
    "            dictionary=dictionary,\n",
    "            documents=documents,\n",
    "            topic_token_weights=topic_token_weights,\n",
    "            topic_token_overview=topic_token_overview,\n",
    "            document_topic_weights=document_topic_weights\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_topic_titles(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id==topic_id)]\n",
    "        df = df_temp\\\n",
    "                .sort_values('weight', ascending=False)\\\n",
    "                .groupby('topic_id')\\\n",
    "                .apply(lambda x: ' '.join(x.token[:n_words].str.title()))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_title(topic_token_weights, topic_id, n_words=100):\n",
    "        return LdaDataCompiler.get_topic_titles(topic_token_weights, topic_id, n_words=n_words).iloc[0]\n",
    "\n",
    "    #get_topics_tokens_as_text = get_topic_titles\n",
    "    #get_topic_tokens_as_text = get_topic_title\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_tokens(topic_token_weights, topic_id=None, n_words=100):\n",
    "        df_temp = topic_token_weights if topic_id is None else topic_token_weights[(topic_token_weights.topic_id == topic_id)]\n",
    "        df = df_temp.sort_values('weight', ascending=False)[:n_words]\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lda_topics(model, n_tokens=20):\n",
    "        return pd.DataFrame({\n",
    "            'Topic#{:02d}'.format(topic_id+1) : [ word[0] for word in model.show_topic(topic_id, topn=n_tokens) ]\n",
    "                for topic_id in range(model.num_topics)\n",
    "        })\n",
    "\n",
    "# OBS OBS! https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "DEFAULT_VECTORIZE_PARAMS = dict(tf_type='linear', apply_idf=False, idf_type='smooth', norm='l2', min_df=1, max_df=0.95)\n",
    "\n",
    "def compute_topic_model(corpus, tick=utility.noop, method='sklearn_lda', vec_args=None, term_args=None, tm_args=None, **args):\n",
    "    \n",
    "    tick()\n",
    "    vec_args = utility.extend({}, DEFAULT_VECTORIZE_PARAMS, vec_args)\n",
    "    \n",
    "    terms_iter = lambda: (textacy_filter_terms(doc, term_args) for doc in corpus)\n",
    "    tick()\n",
    "    \n",
    "    vectorizer = textacy.Vectorizer(**vec_args)\n",
    "    doc_term_matrix = vectorizer.fit_transform(terms_iter())\n",
    "\n",
    "    if method.startswith('sklearn'):\n",
    "        \n",
    "        tm_model = textacy.TopicModel(method.split('_')[1], **tm_args)\n",
    "        tm_model.fit(doc_term_matrix)\n",
    "        tick()\n",
    "        doc_topic_matrix = tm_model.transform(doc_term_matrix)\n",
    "        tick()\n",
    "        tm_id2word = vectorizer.id_to_term\n",
    "        tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        compiled_data = None # FIXME\n",
    "        \n",
    "    elif method.startswith('gensim_'):\n",
    "        \n",
    "        algorithm = method.split('_')[1].upper()\n",
    "        doc_topic_matrix = None # ?\n",
    "        tm_id2word = corpora.Dictionary(terms_iter())\n",
    "        tm_corpus = [ tm_id2word.doc2bow(text) for text in terms_iter() ]\n",
    "        #tm_id2word = vectorizer.id_to_term\n",
    "        #tm_corpus = matutils.Sparse2Corpus(doc_term_matrix, documents_columns=False)\n",
    "        \n",
    "        algorithms = {\n",
    "            'LSI': {\n",
    "                'engine': models.LsiModel,\n",
    "                'options': {\n",
    "                    'corpus': tm_corpus, \n",
    "                    'num_topics':  tm_args.get('n_topics', 0),\n",
    "                    'id2word':  tm_id2word,\n",
    "                    'power_iters': 2,\n",
    "                    'onepass': True\n",
    "                }\n",
    "            },\n",
    "            'LDA': {\n",
    "                'engine': models.LdaModel,\n",
    "                'options': {\n",
    "                    'corpus': tm_corpus, \n",
    "                    'num_topics':  tm_args.get('n_topics', 0),\n",
    "                    'id2word':  tm_id2word,\n",
    "                    'iterations': tm_args.get('max_iter', 0),\n",
    "                    'passes': 20,\n",
    "                    'alpha': 'asymmetric'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        tm_model = algorithms[algorithm]['engine'](**algorithms[algorithm]['options'])\n",
    "        documents = get_corpus_documents(corpus)\n",
    "        compiled_data = LdaDataCompiler.compute_compiled_data(tm_model, tm_corpus, tm_id2word, documents)\n",
    "    \n",
    "    tm_data = types.SimpleNamespace(\n",
    "        tm_model=tm_model,\n",
    "        tm_id2term=tm_id2word,\n",
    "        tm_corpus=tm_corpus,\n",
    "        doc_term_matrix=doc_term_matrix,\n",
    "        doc_topic_matrix=doc_topic_matrix,\n",
    "        vectorizer=vectorizer,\n",
    "        compiled_data=compiled_data\n",
    "    )\n",
    "    \n",
    "    tick(0)\n",
    "    \n",
    "    return tm_data\n",
    "\n",
    "def get_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    topic_ids = range(0,doc_topic_matrix.shape[1])\n",
    "    for document_id in range(0,doc_topic_matrix.shape[1]):\n",
    "        topic_weights = doc_topic_matrix[document_id, :]\n",
    "        for topic_id in topic_ids:\n",
    "            if topic_weights[topic_id] >= threshold:\n",
    "                yield (document_id, topic_id, topic_weights[topic_id])\n",
    "\n",
    "def get_df_doc_topic_weights(doc_topic_matrix, threshold=0.05):\n",
    "    it = get_doc_topic_weights(doc_topic_matrix, threshold)\n",
    "    df = pd.DataFrame(list(it), columns=['document_id', 'topic_id', 'weight']).set_index('document_id')\n",
    "    return df\n",
    "\n",
    "def display_topic_model_gui(corpus, compute_callback):\n",
    "    \n",
    "    pos_options = [ x for x in DF_TAGSET.POS.unique() if x not in ['PUNCT', '', 'DET', 'X', 'SPACE', 'PART', 'CONJ', 'SYM', 'INTJ', 'PRON']]\n",
    "    # groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x)).to_dict()\n",
    "    engine_options = { 'gensim LDA': 'gensim_lda', 'gensim LSI': 'gensim_lsi' } #, 'sklearn_lda': 'sklearn_lda'}\n",
    "    normalize_options = { 'None': False, 'Use lemma': 'lemma', 'Lowercase': 'lower'}\n",
    "    ngrams_options = { '1': [1], '1, 2': [1, 2], '1,2,3': [1, 2, 3] }\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=widgets.Layout(width='90%')),\n",
    "        n_topics=widgets.IntSlider(description='#topics', min=5, max=50, value=20, step=1),\n",
    "        min_freq=widgets.IntSlider(description='Min word freq', min=0, max=10, value=2, step=1),\n",
    "        max_iter=widgets.IntSlider(description='Max iterations', min=100, max=1000, value=20, step=10),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=[1], layout=widgets.Layout(width='200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=widgets.Layout(width='200px')),\n",
    "        filter_stops=widgets.ToggleButton(value=True, description='Remove stopword',  tooltip='Filter out stopwords', icon='check'),\n",
    "        filter_nums=widgets.ToggleButton(value=True, description='Remove nums',  tooltip='Filter out stopwords', icon='check'),\n",
    "        named_entities=widgets.ToggleButton(value=False, description='Merge entities',  tooltip='Merge entities', icon='check'),\n",
    "        drop_determiners=widgets.ToggleButton(value=True, description='Drop determiners',  tooltip='Drop determiners', icon='check'),\n",
    "        apply_idf=widgets.ToggleButton(value=False, description='Apply IDF',  tooltip='Apply TF-IDF', icon='check'),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=['NOUN', 'PROPN'], rows=7, layout=widgets.Layout(width='200px')),\n",
    "        method=widgets.Dropdown(description='Engine', options=engine_options, value='gensim_lda', layout=widgets.Layout(width='200px')),\n",
    "        compute=widgets.Button(description='Compute'),\n",
    "        boxes=None,\n",
    "        output = widgets.Output(), # layout={'height': '500px'}),\n",
    "        model=None\n",
    "    )\n",
    "    gui.boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.n_topics,\n",
    "                gui.min_freq,\n",
    "                gui.max_iter\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.filter_stops,\n",
    "                gui.filter_nums,\n",
    "                gui.named_entities,\n",
    "                gui.drop_determiners,\n",
    "                gui.apply_idf\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.method\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            widgets.VBox([\n",
    "                gui.compute\n",
    "            ])\n",
    "        ]),\n",
    "        widgets.VBox([gui.output]), # ,layout=widgets.Layout(top='20px', height='500px',width='100%'))\n",
    "    ])\n",
    "    fx = lambda *args: compute_callback(corpus, gui, *args)\n",
    "    gui.compute.on_click(fx)\n",
    "    return gui\n",
    "    \n",
    "\n",
    "def compute_callback(corpus, gui, *args):\n",
    "\n",
    "    def tick(x=None):\n",
    "        gui.progress.value = gui.progress.value + 1 if x is None else x\n",
    "    \n",
    "    tick(1)\n",
    "    gui.output.clear_output()\n",
    "\n",
    "    with gui.output:\n",
    "        vec_args = dict(apply_idf=gui.apply_idf.value)\n",
    "        term_args = dict(\n",
    "            args=dict(\n",
    "                ngrams=gui.ngrams.value,\n",
    "                named_entities=gui.named_entities.value,\n",
    "                normalize=gui.normalize.value,\n",
    "                as_strings=True\n",
    "            ),\n",
    "            kwargs=dict(\n",
    "                filter_nums=gui.filter_nums.value,\n",
    "                drop_determiners=gui.drop_determiners.value,\n",
    "                min_freq=gui.min_freq.value,\n",
    "                include_pos=gui.include_pos.value,\n",
    "                filter_stops=gui.filter_stops.value,\n",
    "                filter_punct=True\n",
    "            )\n",
    "        )\n",
    "        tm_args = dict(\n",
    "            n_topics=gui.n_topics.value,\n",
    "            max_iter=gui.max_iter.value,\n",
    "            learning_method='online', \n",
    "            n_jobs=1\n",
    "        )\n",
    "        method = gui.method.value\n",
    "        gui.model = compute_topic_model(\n",
    "            corpus=corpus,\n",
    "            tick=tick,\n",
    "            method=method,\n",
    "            vec_args=vec_args,\n",
    "            term_args=term_args,\n",
    "            tm_args=tm_args\n",
    "        )\n",
    "    \n",
    "    gui.output.clear_output()\n",
    "    with gui.output:\n",
    "        display(LdaDataCompiler.get_lda_topics(gui.model.tm_model, n_tokens=20))\n",
    "        \n",
    "TM_GUI_MODEL = display_topic_model_gui(CURRENT_CORPUS.textacy_corpus, compute_callback)\n",
    "display(TM_GUI_MODEL.boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display Named Entities\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus):\n",
    "    \n",
    "    def display_document_entities(document_id, corpus):\n",
    "        displacy.render(corpus[document_id].spacy_doc, style='ent', jupyter=True)\n",
    "    \n",
    "    df_documents = get_corpus_documents(corpus)\n",
    "\n",
    "    document_widget = widgets.Dropdown(description='Paper', options={v: k for k, v in df_documents['title'].to_dict().items()}, value=0, layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(display_document_entities,document_id=document_widget, corpus=widgets.fixed(corpus))\n",
    "\n",
    "    display(widgets.VBox([document_widget, widgets.VBox([itw.children[-1]],layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    display_document_entities_gui(CURRENT_CORPUS.textacy_corpus)\n",
    "except Except as ex:\n",
    "    logger.error(ec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Wordcloud<span style='color: red; float: right'>TRY IT</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Display LDA topic's token wordcloud\n",
    "opts = { 'max_font_size': 100, 'background_color': 'white', 'width': 900, 'height': 600 }\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import common.widgets_utility as widgets_utility\n",
    "\n",
    "def display_wordcloud_gui(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    model = tm_data.tm_model\n",
    "    output_options = output_options or []\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "def plot_wordcloud(df_data, token='token', weight='weight', figsize=(14, 14/1.618), **args):\n",
    "    token_weights = dict({ tuple(x) for x in df_data[[token, weight]].values })\n",
    "    image = wordcloud.WordCloud(**args,)\n",
    "    image.fit_words(token_weights)\n",
    "    plt.figure(figsize=figsize) #, dpi=100)\n",
    "    plt.imshow(image, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def display_wordcloud(\n",
    "    tm_data,\n",
    "    topic_id=0,\n",
    "    n_words=100,\n",
    "    output_format='Wordcloud',\n",
    "    widget_container=None\n",
    "):\n",
    "    container = tm_data.compiled_data\n",
    "    widget_container.progress.value = 1\n",
    "    df_temp = container.topic_token_weights.loc[(container.topic_token_weights.topic_id == topic_id)]\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=n_words)\n",
    "    widget_container.value = 2\n",
    "    widget_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    if output_format == 'Wordcloud':\n",
    "        plot_wordcloud(df_temp, 'token', 'weight', max_words=n_words, **opts)\n",
    "    elif output_format == 'Table':\n",
    "        widget_container.progress.value = 3\n",
    "        df_temp = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id, n_words=n_words)\n",
    "        widget_container.progress.value = 4\n",
    "        display(HTML(df_temp.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(LdaDataCompiler.get_topic_tokens(topic_id, n_words)))\n",
    "    widget_container.progress.value = 0\n",
    "\n",
    "try:\n",
    "    tm_data = get_current_model()\n",
    "    display_wordcloud_gui(display_wordcloud, tm_data, 'tx02', ['Wordcloud', 'Table', 'Pivot'])\n",
    "except TopicModelNotComputed as ex:\n",
    "    logger.info(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Word Distribution as a Chart<span style='color: red; float: right'>TRY IT</span>\n",
    "The following chart shows the word distribution for each selected topic. You can zoom in on the left chart. The distribution seems to follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) as (perhaps) expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Display topic's word distribution\n",
    "import numpy as np\n",
    "\n",
    "def plot_topic_word_distribution(tokens, **args):\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens)\n",
    "\n",
    "    p = bokeh.plotting.figure(toolbar_location=\"right\", **args)\n",
    "\n",
    "    cr = p.circle(x='xs', y='ys', source=source)\n",
    "\n",
    "    label_style = dict(level='overlay', text_font_size='8pt', angle=np.pi/6.0)\n",
    "\n",
    "    text_aligns = ['left', 'right']\n",
    "    for i in [0, 1]:\n",
    "        label_source = bokeh.models.ColumnDataSource(tokens.iloc[i::2])\n",
    "        labels = bokeh.models.LabelSet(x='xs', y='ys', text_align=text_aligns[i], text='token', text_baseline='middle',\n",
    "                          y_offset=5*(1 if i == 0 else -1),\n",
    "                          x_offset=5*(1 if i == 0 else -1),\n",
    "                          source=label_source, **label_style)\n",
    "        p.add_layout(labels)\n",
    "\n",
    "    p.xaxis[0].axis_label = 'Token #'\n",
    "    p.yaxis[0].axis_label = 'Probability%'\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"6pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    return p\n",
    "\n",
    "def display_topic_tokens(tm_data, topic_id=0, n_words=100, output_format='Chart', widget_container=None):\n",
    "    widget_container.forward()\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_tokens(container.topic_token_weights, topic_id=topic_id).\\\n",
    "        copy()\\\n",
    "        .drop('topic_id', axis=1)\\\n",
    "        .assign(weight=lambda x: 100.0 * x.weight)\\\n",
    "        .sort_values('weight', axis=0, ascending=False)\\\n",
    "        .reset_index()\\\n",
    "        .head(n_words)\n",
    "    if output_format == 'Chart':\n",
    "        widget_container.forward()\n",
    "        tokens = tokens.assign(xs=tokens.index, ys=tokens.weight)\n",
    "        p = plot_topic_word_distribution(tokens, plot_width=1000, plot_height=500, title='', tools='box_zoom,wheel_zoom,pan,reset')\n",
    "        bokeh.plotting.show(p)\n",
    "        widget_container.forward()\n",
    "    elif output_format == 'Table':\n",
    "        #display(tokens)\n",
    "        display(HTML(tokens.to_html()))\n",
    "    else:\n",
    "        display(pivot_ui(tokens))\n",
    "        \n",
    "    # Added code for missing method: widget_container.reset()\n",
    "    if 'progress' in widget_container.__dict__.keys():\n",
    "        widget_container.progress.value = 0\n",
    "    \n",
    "    \n",
    "def display_topic_distribution_widgets(callback, tm_data, text_id, output_options=None, word_count=(1, 100, 50)):\n",
    "    \n",
    "    output_options = output_options or []\n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id=text_id,\n",
    "        text=wf.create_text_widget(text_id),\n",
    "        topic_id=widgets.IntSlider(\n",
    "            description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        word_count=widgets.IntSlider(\n",
    "            description='#Words', min=word_count[0], max=word_count[1], step=1, value=word_count[2], continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', output_options, default=output_options[0], layout=widgets.Layout(width=\"200px\")),\n",
    "        progress = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"95%\"))\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        callback,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        topic_id=wc.topic_id,\n",
    "        n_words=wc.word_count,\n",
    "        output_format=wc.output_format,\n",
    "        widget_container=widgets.fixed(wc)\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.topic_id, wc.word_count, wc.output_format]),\n",
    "        wc.progress,\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "\n",
    "    iw.update()\n",
    "TM_DATA = TM_GUI_MODEL.model\n",
    "\n",
    "display_topic_distribution_widgets(display_topic_tokens, TM_DATA, 'wc01', ['Chart', 'Table'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic's Trend Over Time or Documents<span style='color: red; float: right'>RUN</span>\n",
    "- Displays topic's share over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a topic's yearly weight over time in selected LDA topic model\n",
    "#import numpy as np\n",
    "#import math\n",
    "#import bokeh.plotting\n",
    "#from bokeh.models import ColumnDataSource, DataRange1d, Plot, LinearAxis, Grid\n",
    "#from bokeh.models.glyphs import VBar\n",
    "#from bokeh.io import curdoc, show\n",
    "\n",
    "def slim_title(x):\n",
    "    try:\n",
    "        m = re.match('.*\\((.*)\\)$', x).groups()\n",
    "        if m is not None and len(m) > 0:\n",
    "            return m[0]\n",
    "        return ' '.join(x.split(' ')[:3]) + '...'\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "import math\n",
    "\n",
    "def plot_topic_trend(df, pivot_column, value_column, x_label=None, y_label=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,previewsave\"\n",
    "\n",
    "    xs = df[pivot_column].astype(np.str)\n",
    "    p = bokeh.plotting.figure(x_range=xs, plot_width=1000, plot_height=700, title='', tools=tools, toolbar_location=\"right\")\n",
    "\n",
    "    glyph = p.vbar(x=xs, top=df[value_column], width=0.5, fill_color=\"#b3de69\")\n",
    "    p.xaxis.major_label_orientation = math.pi/4\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.xaxis[0].axis_label = (x_label or '').title()\n",
    "    p.yaxis[0].axis_label = (y_label or '').title()\n",
    "    p.y_range.start = 0.0\n",
    "    #p.y_range.end = 1.0\n",
    "    p.x_range.range_padding = 0.01\n",
    "    return p\n",
    "\n",
    "def display_topic_trend(topic_id, widgets_container, output_format='Chart', tm_data=None, threshold=0.01):\n",
    "    container = tm_data.compiled_data\n",
    "    tokens = LdaDataCompiler.get_topic_title(container.topic_token_weights, topic_id, n_words=200)\n",
    "    widgets_container.text.value = 'ID {}: {}'.format(topic_id, tokens)\n",
    "    value_column = 'weight'\n",
    "    category_column = 'author'\n",
    "    df = container.document_topic_weights[(container.document_topic_weights.topic_id==topic_id)]\n",
    "    df = df[(df.weight > threshold)].reset_index()\n",
    "    df[category_column] = df.title.apply(slim_title)\n",
    "\n",
    "    if output_format == 'Table':\n",
    "        display(df)\n",
    "    else:\n",
    "        x_label = category_column.title()\n",
    "        y_label = value_column.title()\n",
    "        p = plot_topic_trend(df, category_column, value_column, x_label=x_label, y_label=y_label)\n",
    "        bokeh.plotting.show(p)\n",
    "\n",
    "def create_topic_trend_widgets(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    wf = widgets_utility.wf\n",
    "    wc = widgets_utility.WidgetUtility(\n",
    "        n_topics=model.num_topics,\n",
    "        text_id='topic_share_plot',\n",
    "        text=wf.create_text_widget('topic_share_plot'),\n",
    "        threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=0.25, step=0.01, value=0.10, continuous_update=False),\n",
    "        topic_id=widgets.IntSlider(description='Topic ID', min=0, max=model.num_topics - 1, step=1, value=0, continuous_update=False),\n",
    "        output_format=wf.create_select_widget('Format', ['Chart', 'Table'], default='Chart'),\n",
    "        progress=widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    )\n",
    "\n",
    "    wc.prev_topic_id = wc.create_prev_id_button('topic_id', model.num_topics)\n",
    "    wc.next_topic_id = wc.create_next_id_button('topic_id', model.num_topics)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_trend,\n",
    "        topic_id=wc.topic_id,\n",
    "        widgets_container=widgets.fixed(wc),\n",
    "        output_format=wc.output_format,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=wc.threshold\n",
    "    )\n",
    "    display(widgets.VBox([\n",
    "        wc.text,\n",
    "        widgets.HBox([wc.prev_topic_id, wc.next_topic_id, wc.output_format]),\n",
    "        widgets.HBox([wc.topic_id, wc.threshold, wc.progress]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    \n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "create_topic_trend_widgets(tm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Display Topic to Document Network<span style='color: red; float: right'>TRY IT</span>\n",
    "The green nodes are documents, and blue nodes are topics. The edges (lines) indicates the strength of a topic in the connected document. The width of the edge is proportinal to the strength of the connection. Note that only edges with a strength above the certain threshold are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Visualize year-to-topic network by means of topic-document-weights\n",
    "from common.plot_utility import layout_algorithms, PlotNetworkUtility\n",
    "from common.network_utility import NetworkUtility, DISTANCE_METRICS, NetworkMetricHelper\n",
    "\n",
    "def plot_document_topic_network(network, layout, scale=1.0, titles=None):\n",
    "    tools = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n",
    "    year_nodes, topic_nodes = NetworkUtility.get_bipartite_node_set(network, bipartite=0)  \n",
    "    \n",
    "    year_source = NetworkUtility.get_node_subset_source(network, layout, year_nodes)\n",
    "    topic_source = NetworkUtility.get_node_subset_source(network, layout, topic_nodes)\n",
    "    lines_source = NetworkUtility.get_edges_source(network, layout, scale=6.0, normalize=False)\n",
    "    \n",
    "    edges_alphas = NetworkMetricHelper.compute_alpha_vector(lines_source.data['weights'])\n",
    "    \n",
    "    lines_source.add(edges_alphas, 'alphas')\n",
    "    \n",
    "    p = bokeh.plotting.figure(plot_width=1000, plot_height=600, x_axis_type=None, y_axis_type=None, tools=tools)\n",
    "    \n",
    "    r_lines = p.multi_line(\n",
    "        'xs', 'ys', line_width='weights', alpha='alphas', color='black', source=lines_source\n",
    "    )\n",
    "    r_years = p.circle(\n",
    "        'x','y', size=40, source=year_source, color='lightgreen', level='overlay', line_width=1,alpha=1.0\n",
    "    )\n",
    "    \n",
    "    r_topics = p.circle('x','y', size=25, source=topic_source, color='skyblue', level='overlay', alpha=1.00)\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(renderers=[r_topics], tooltips=None, callback=widgets_utility.wf.\\\n",
    "        glyph_hover_callback(topic_source, 'node_id', text_ids=titles.index, text=titles, element_id='nx_id1'))\n",
    "    )\n",
    "\n",
    "    text_opts = dict(x='x', y='y', text='name', level='overlay', x_offset=0, y_offset=0, text_font_size='8pt')\n",
    "    \n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=year_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    p.add_layout(\n",
    "        bokeh.models.LabelSet(\n",
    "            source=topic_source, text_color='black', text_align='center', text_baseline='middle', **text_opts\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "\n",
    "def main_topic_network(tm_data):\n",
    "    \n",
    "    model = tm_data.tm_model\n",
    "    text_id = 'nx_id1'\n",
    "    layout_options = [ 'Circular', 'Kamada-Kawai', 'Fruchterman-Reingold']\n",
    "    text_widget = widgets_utility.wf.create_text_widget(text_id)  # style=\"display: inline; height='400px'\"),\n",
    "    scale_widget = widgets.FloatSlider(description='Scale', min=0.0, max=1.0, step=0.01, value=0.1, continues_update=False)\n",
    "    threshold_widget = widgets.FloatSlider(description='Threshold', min=0.0, max=1.0, step=0.01, value=0.50, continues_update=False)\n",
    "    output_format_widget = widgets_utility.dropdown('Output', { 'Network': 'network', 'Table': 'table' }, 'network')\n",
    "    layout_widget = widgets_utility.dropdown('Layout', layout_options, 'Fruchterman-Reingold')\n",
    "    progress_widget = widgets.IntProgress(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"40%\"))\n",
    "    \n",
    "    def tick(x=None):\n",
    "        progress_widget.value = progress_widget.value + 1 if x is None else x\n",
    "        \n",
    "    def display_topic_network(layout_algorithm, tm_data, threshold=0.10, scale=1.0, output_format='network'):\n",
    "            \n",
    "        tick(1)\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights)\n",
    "\n",
    "        df = container.document_topic_weights[container.document_topic_weights.weight > threshold].reset_index()\n",
    "        \n",
    "        df['slim_title'] = df.title.apply(slim_title)\n",
    "        network = NetworkUtility.create_bipartite_network(df, 'slim_title', 'topic_id')\n",
    "        \n",
    "        tick()\n",
    "\n",
    "        if output_format == 'network':\n",
    "            \n",
    "            args = PlotNetworkUtility.layout_args(layout_algorithm, network, scale)\n",
    "            layout = (layout_algorithms[layout_algorithm])(network, **args)\n",
    "            \n",
    "            tick()\n",
    "            \n",
    "            p = plot_document_topic_network(network, layout, scale=scale, titles=titles)\n",
    "            bokeh.plotting.show(p)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(df)\n",
    "        else:\n",
    "            display(pivot_ui(df))\n",
    "\n",
    "        tick(0)\n",
    "\n",
    "    iw = widgets.interactive(\n",
    "        display_topic_network,\n",
    "        layout_algorithm=layout_widget,\n",
    "        tm_data=widgets.fixed(tm_data),\n",
    "        threshold=threshold_widget,\n",
    "        scale=scale_widget,\n",
    "        output_format=output_format_widget\n",
    "    )\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        text_widget,\n",
    "        widgets.HBox([layout_widget, threshold_widget]), \n",
    "        widgets.HBox([output_format_widget, scale_widget, progress_widget]),\n",
    "        iw.children[-1]\n",
    "    ]))\n",
    "    iw.update()\n",
    "\n",
    "tm_data = get_current_model()\n",
    "main_topic_network(tm_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>VISUALIZE</span> Topic Trends Overview<span style='color: red; float: right'>TRY IT</span>\n",
    "\n",
    "- The topic shares  displayed as a scattered heatmap plot using gradient color based on topic's weight in document.\n",
    "- [Stanfords Termite software](http://vis.stanford.edu/papers/termite) uses a similar visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_topic_relevance_by_year\n",
    "import bokeh.transform\n",
    "\n",
    "def setup_glyph_coloring(df):\n",
    "    max_weight = df.weight.max()\n",
    "    #colors = list(reversed(bokeh.palettes.Greens[9]))\n",
    "    colors = ['#ffffff', '#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']\n",
    "    mapper = bokeh.models.LinearColorMapper(palette=colors, low=0.0, high=1.0) # low=df.weight.min(), high=max_weight)\n",
    "    color_transform = bokeh.transform.transform('weight', mapper)\n",
    "    color_bar = bokeh.models.ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=bokeh.models.BasicTicker(desired_num_ticks=len(colors)),\n",
    "                         formatter=bokeh.models.PrintfTickFormatter(format=\" %5.2f\"))\n",
    "    return color_transform, color_bar\n",
    "\n",
    "def plot_topic_relevance_by_year(df, xs, ys, flip_axis, glyph, titles, text_id):\n",
    "\n",
    "    line_height = 7\n",
    "    if flip_axis is True:\n",
    "        xs, ys = ys, xs\n",
    "        line_height = 10\n",
    "    \n",
    "    ''' Setup axis categories '''\n",
    "    x_range = list(map(str, df[xs].unique()))\n",
    "    y_range = list(map(str, df[ys].unique()))\n",
    "    \n",
    "    ''' Setup coloring and color bar '''\n",
    "    color_transform, color_bar = setup_glyph_coloring(df)\n",
    "    \n",
    "    source = bokeh.models.ColumnDataSource(df)\n",
    "\n",
    "    plot_height = max(len(y_range) * line_height, 500)\n",
    "    \n",
    "    p = bokeh.plotting.figure(title=\"Topic heatmap\", toolbar_location=\"right\", x_range=x_range,\n",
    "           y_range=y_range, x_axis_location=\"above\", plot_width=1000, plot_height=plot_height)\n",
    "\n",
    "    args = dict(x=xs, y=ys, source=source, alpha=1.0, hover_color='red')\n",
    "    \n",
    "    if glyph == 'Circle':\n",
    "        cr = p.circle(color=color_transform, **args)\n",
    "    else:\n",
    "        cr = p.rect(width=1, height=1, line_color=None, fill_color=color_transform, **args)\n",
    "\n",
    "    p.x_range.range_padding = 0\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"8pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    \n",
    "    p.add_tools(bokeh.models.HoverTool(tooltips=None, callback=widgets_utility.WidgetUtility.glyph_hover_callback(\n",
    "        source, 'topic_id', titles.index, titles, text_id), renderers=[cr]))\n",
    "    \n",
    "    return p\n",
    "    \n",
    "def display_doc_topic_heatmap(tm_data, key='max', flip_axis=False, glyph='Circle'):\n",
    "    try:\n",
    "        container = tm_data.compiled_data\n",
    "        titles = LdaDataCompiler.get_topic_titles(container.topic_token_weights, n_words=100)\n",
    "        df = container.document_topic_weights.copy().reset_index()\n",
    "        df['document_id'] = df.index.astype(str)\n",
    "        df['topic_id'] = df.topic_id.astype(str)\n",
    "        df['author'] = df.title.apply(slim_title)\n",
    "        p = plot_topic_relevance_by_year(df, xs='author', ys='topic_id', flip_axis=flip_axis, glyph=glyph, titles=titles, text_id='topic_relevance')\n",
    "        bokeh.plotting.show(p)\n",
    "    except Exception as ex:\n",
    "        raise\n",
    "        logger.error(ex)\n",
    "            \n",
    "def doc_topic_heatmap_gui(tm_data):\n",
    "    \n",
    "    def text_widget(element_id=None, default_value='', style='', line_height='20px'):\n",
    "        value = \"<span class='{}' style='line-height: {};{}'>{}</span>\".format(element_id, line_height, style, default_value) if element_id is not None else ''\n",
    "        return widgets.HTML(value=value, placeholder='', description='', layout=widgets.Layout(height='150px'))\n",
    "\n",
    "    text_id = 'topic_relevance'\n",
    "    #text_widget = widgets_utility.wf.create_text_widget(text_id)\n",
    "    text_widget = text_widget(text_id)\n",
    "    glyph = widgets.Dropdown(options=['Circle', 'Square'], value='Square', description='Glyph', layout=widgets.Layout(width=\"180px\"))\n",
    "    flip_axis = widgets.ToggleButton(value=True, description='Flip XY', tooltip='Flip X and Y axis', icon='', layout=widgets.Layout(width=\"80px\"))\n",
    "\n",
    "    iw = widgets.interactive(display_doc_topic_heatmap, tm_data=widgets.fixed(tm_data), glyph=glyph, flip_axis=flip_axis)\n",
    "\n",
    "    display(widgets.VBox([widgets.HBox([flip_axis, glyph ]), text_widget, iw.children[-1]]))\n",
    "\n",
    "    iw.update()\n",
    "\n",
    "doc_topic_heatmap_gui(get_current_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> CO-occurrence Matrix<span style='color: red; float: right'>WORK IN PROGRESS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def corpus_subset(corpus, px):\n",
    "    iter1, iter2 = itertools.tee(corpus.get(px))\n",
    "    docs = (d.spacy_doc for d in iter1)\n",
    "    metadatas = (d.metadata for d in iter2)\n",
    "    sub_corpus = textacy.Corpus(lang=corpus.spacy_lang, docs=docs, metadatas=metadatas)\n",
    "    return sub_corpus\n",
    "\n",
    "def fpx(year):\n",
    "    return lambda x: int(x.metadata['signed_year']) == year\n",
    "\n",
    "#data = corpus_subset(corpus, fpx(1947)).word_freqs(weighting='freq', as_strings=True)\n",
    "#df = pd.DataFrame({'key': list(data.keys()),  'weight': list(data.values()) })\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from glove import Corpus\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# See http://www.foldl.me/2014/glove-python/\n",
    "#def compute_GloVe_df(sentences, window=2, dictionary=None):\n",
    "#    \n",
    "#    corpus = Corpus(dictionary=dictionary)\n",
    "#    corpus.fit(sentences, window=window)\n",
    "\n",
    "#    dm = corpus.matrix.todense()\n",
    "#    inverse_dictionary = { i: w for w, i in corpus.dictionary.items() }\n",
    "#    id2token = [ inverse_dictionary[i] for i in range(0,max(inverse_dictionary.keys())+1)]\n",
    "\n",
    "#    df = pd.DataFrame(dm.T, columns=id2token).assign(word=id2token).set_index('word')\n",
    "#    return df\n",
    "\n",
    "#https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.nonzero.html\n",
    "    \n",
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    # A = scipy.sparse.triu(A)\n",
    "    s = pd.Series(A.data, pd.MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()\n",
    "    return s\n",
    "\n",
    "term_args = dict(\n",
    "    args=dict(\n",
    "        ngrams=1,\n",
    "        named_entities=False,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        filter_stops=True,\n",
    "        filter_punct=True,\n",
    "        filter_nums=True,\n",
    "        min_freq=1,\n",
    "        drop_determiners=True,\n",
    "        include_pos=('NOUN', 'PROPN', )\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = (textacy_filter_terms(doc, term_args) for doc in CORPUS)\n",
    "#stream = (' '.join(list(textacy_filter_terms(doc, term_args))) for doc in CORPUS)\n",
    "\n",
    "glove_co_matrix = Corpus() #dictionary=None)\n",
    "\n",
    "docs = (list(textacy_filter_terms(doc, term_args)) for doc in CORPUS)\n",
    "glove_co_matrix.fit(docs, window=5)\n",
    "\n",
    "dictionary = glove_co_matrix.dictionary\n",
    "co_series = _coo_to_sparse_series(glove_co_matrix.matrix)\n",
    "co_df = co_series.to_dense().reset_index().rename(columns={ 'level_0': 'token_id_x', 'level_1': 'token_id_y', 0: 'weight'})\n",
    "id2token = pd.DataFrame({ 'token_id': list(dictionary.values()), 'token': list(dictionary.keys()) }).set_index('token_id')\n",
    "\n",
    "#https://stackoverflow.com/questions/34181494/populate-a-pandas-sparsedataframe-from-a-scipy-sparse-coo-matrix\n",
    "\n",
    "df = pd.merge(co_df, id2token, left_on='token_id_x', right_index=True, how='inner').rename(columns={'token': 'token_x'})\n",
    "df = pd.merge(df,    id2token, left_on='token_id_y', right_index=True, how='inner').rename(columns={'token': 'token_y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'row': glove_co_matrix.matrix.row, 'col': glove_co_matrix.matrix.col}).groupby(['row', 'col']).size().nlargest()\n",
    "#glove_co_matrix.matrix.tocoo()\n",
    "#df.nlargest(100, ['weight'])\n",
    "#df.groupby(['token_id_x', 'token_id_y']).size().nlargest()\n",
    "#co_series.reset_index().groupby(['level_0', 'level_1']).size().nlargest()\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS AND WORK IN PROGRESS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame Corpus from spaCy Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_statistics(spacy_doc):\n",
    "    \n",
    "    spacy_sentences = list(spacy_doc.sents)\n",
    "    token_length_histogram = collections.Counter((len(x) for x in spacy_doc))\n",
    "    \n",
    "    return {\n",
    "        'word_count': len(spacy_doc),\n",
    "        'sentence_count': len(spacy_sentences),\n",
    "        'avg_words_per_sentence': len(spacy_doc) / len(spacy_sentences),\n",
    "        'token_lengths': dict(token_length_histogram)\n",
    "    }\n",
    "\n",
    "def doc_to_dataframe(treaty_id, spacy_doc, include_pos=None, ignore_pos=None):\n",
    "    include_pos = include_pos or []\n",
    "    ignore_pos = ignore_pos or []\n",
    "    df_source = ({\n",
    "        \"treaty_id\": treaty_id,\n",
    "        \"text\": w.text.lower(),\n",
    "        \"lemma\": w.lemma_.lower(),\n",
    "        \"pos\": w.pos_,\n",
    "        # \"tag\": w.tag_,\n",
    "        # \"dep\": w.dep_,\n",
    "        \"is_alpha\": w.is_alpha,\n",
    "        \"is_stop\": w.is_stop\n",
    "    } for w in spacy_doc\n",
    "        if w.pos_ in include_pos\n",
    "        and w.pos_ not in ignore_pos)\n",
    "    return pd.DataFrame(df_source)\n",
    "\n",
    "def create_dataframe_corpus(corpus, include_pos=None, ignore_pos=None, ticker=utility.noop):\n",
    "    \n",
    "    df_corpus = None\n",
    "    try:\n",
    "        for doc in corpus:\n",
    "            df = doc_to_dataframe(doc.metadata['treaty_id'], doc.spacy_doc, include_pos=include_pos, ignore_pos=ignore_pos)\n",
    "            if df_corpus is None:\n",
    "                df_corpus = df\n",
    "            else:\n",
    "                df_corpus = df_corpus.append(df, ignore_index=True)\n",
    "            tick()\n",
    "    finally:\n",
    "        tick(0)\n",
    "    return df_corpus\n",
    "\n",
    "progress_widget = widgets.IntProgress(min=0,max=len(CORPUS),step=1, layout=widgets.Layout(width='95%'))\n",
    "display(progress_widget)\n",
    "\n",
    "def ticker(w):\n",
    "    def tick(x=None):\n",
    "        w.value = w.value + 1 if x is None else x\n",
    "    return tick\n",
    "\n",
    "tick = ticker(progress_widget)\n",
    "df_corpus = create_dataframe_corpus(CORPUS, include_pos=('NOUN', 'PROPN', 'VERB'), ticker=tick)\n",
    "df_corpus.to_csv('spaCy_PoS_corpus.csv', sep='\\t', encoding='utf-8') # , compression='zip')\n",
    "df_documents = pd.DataFrame([ x.metadata for x in CORPUS]).set_index('treaty_id')\n",
    "# df_corpus2 = pd.read_csv('spaCy_PoS_corpus.csv', index_col=0, sep='\\t', encoding='utf-8')\n",
    "df = df_corpus.merge(df_documents, left_on='treaty_id', right_index=True, how='inner')\n",
    "df.loc[df.lemma.isin(['culture', 'cultural'])].groupby(['signed_year']).size().plot(kind='bar')\n",
    "\n",
    "df_treaty_pos_tokens = df_corpus[~df_corpus.Stop&~df_corpus.POS.isin(['SPACE', 'PUNCT'])].groupby(['treaty_id', 'Lemma', 'POS']).size()\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.reset_index().rename(columns={'Lemma': 'lemma', 'POS': 'pos', 0: 'count'})\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.set_index(['treaty_id'])\n",
    "df_treaty_pos_tokens = df_treaty_pos_tokens.merge(pd.DataFrame(treaties['signed_year']), how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_year_pos_tokens = df.groupby(['lemma', 'pos']).size().reset_index().rename(columns={0:'count'})\n",
    "df_year_pos_tokens[df_year_pos_tokens.pos.isin(['NOUN'])].nlargest(50, 'count')\n",
    "#df_year_pos_tokens\n",
    "\n",
    "df_token_pos = df_corpus[~df_corpus.POS.isin(['SPACE', 'PUNCT'])].groupby(['Lemma', 'POS']).size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus vs WTI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_documents = corpus.documents.set_index(['treaty_id', 'language'])\n",
    "treaty_text_languages = wti_index.get_treaty_text_languages().set_index(['treaty_id', 'language'])\n",
    "\n",
    "treaties_in_corpus_not_in_wti = corpus_documents.index.difference(treaty_text_languages.index).get_values()\n",
    "treaties_in_wti_not_in_corpus = treaty_text_languages.index.difference(corpus_documents.index).get_values()\n",
    "\n",
    "print(  'Found in corpus, but not in WTI: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_corpus_not_in_wti ]))\n",
    "\n",
    "print(  'Found in WTI, but not in corpus: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_wti_not_in_corpus ]))\n",
    "\n",
    "#corpus_documents.loc[corpus_text_not_in_wti]\n",
    "#treaty_text_languages.loc[wti_not_in_corpus]\n",
    "\n",
    "#wti_not_in_corpus\n",
    "\n",
    "# Duplicates:\n",
    "#corpus_documents.index.get_duplicates()\n",
    "#treaty_text_languages.corpus_documents.index.get_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics\n",
    "See https://www.nltk.org/book/ch01.html\n",
    "\n",
    "* Size of treaties over time\n",
    "* Unique word, unique words per word class\n",
    "* Lexical diversity\n",
    "* Frequency distribution\n",
    "* Average word length, sentence length\n",
    "\n",
    "\n",
    "```python\n",
    " \n",
    "len(texts) / count(docs)\n",
    "0.06230453042623537\n",
    "len(set(text3)) / len(text3)\n",
    "0.06230453042623537\n",
    "def lexical_diversity(text): [1]\n",
    "    return len(set(text)) / len(text) [2]\n",
    "\n",
    "def percentage(count, total): [3]\n",
    "    return 100 * count / total\n",
    "#### Most common words\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)\n",
    "#### Word length frequencies\n",
    "fdist = FreqDist(len(w) for w in text1)  [2]\n",
    "print(fdist)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist_interact(source_folder):\n",
    "    global corpus\n",
    "    progress_widget = None\n",
    "    \n",
    "    def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "        global corpus\n",
    "\n",
    "        try:\n",
    "\n",
    "            progress_widget.value = 1\n",
    "\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "            progress_widget.value = 2\n",
    "            service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "            print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                      .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "            progress_widget.value = 3\n",
    "            if statistics == 'word_freqs':\n",
    "                display(service.compute_word_frequencies(remove_stopwords))\n",
    "            elif statistics == 'documents':\n",
    "                display(service.compute_document_stats())\n",
    "            elif statistics == 'word_count':\n",
    "                display(service.compute_word_stats())\n",
    "            else:\n",
    "                print('Unknown: ' + statistics)\n",
    "\n",
    "        except Exception as ex:\n",
    "            logger.error(ex)\n",
    "\n",
    "        progress_widget.value = 5\n",
    "        progress_widget.value = 0\n",
    "        return corpus\n",
    "    \n",
    "    language_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    statistics_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    remove_stopwords_widget=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist'\n",
    "    )\n",
    "    \n",
    "    progress_widget=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    "\n",
    "    wi = widgets.interactive(\n",
    "        display_token_toplist,\n",
    "        source_folder=source_folder,\n",
    "        language=language_widget,\n",
    "        statistics=statistics_widget,\n",
    "        remove_stopwords=remove_stopwords_widget\n",
    "    )\n",
    "\n",
    "    boxes = widgets.HBox(\n",
    "        [\n",
    "            language_widget, statistics_widget, remove_stopwords_widget, progress_widget\n",
    "        ]\n",
    "    )\n",
    "    display(widgets.VBox([boxes, wi.children[-1]]))\n",
    "    wi.update()\n",
    "\n",
    "display_token_toplist_interact('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Sprck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        display(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "model = Cooccurrence(ngram_range=(1, 1))\n",
    "Xc = model.fit_transform(corpus)\n",
    "\n",
    "id2token = { i: x for (i, x) in enumerate(model.get_feature_names()) }\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Xxy = Xc.tocoo()\n",
    "word1 = [ id2token[x] for x in Xxy.row ]\n",
    "word2 = [ id2token[x] for x in Xxy.col ]\n",
    "\n",
    "df = pd.DataFrame({ 'word1': [ id2token[x] for x in Xxy.row ], 'word2': [ id2token[x] for x in Xxy.col ], 'count': Xxy.data })[['word1', 'word2', 'count']] #.set_index(['word1', 'word2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       one   two\n",
       "word            \n",
       "one    0.0  23.5\n",
       "two   23.5   0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    s = Series(A.data, MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()  # TODO: specify kind?\n",
    "    # ...\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
